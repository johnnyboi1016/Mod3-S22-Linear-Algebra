{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T19:13:50.339427Z",
     "start_time": "2020-02-26T19:13:50.329438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2],\n",
       "        [3, 4]],\n",
       "\n",
       "       [[5, 6],\n",
       "        [7, 8]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SLICING ARRAYS\n",
    "import numpy as np\n",
    "arr = np.array([[[1,2],[3,4]], [[5,6],[7,8]]]) #3d array\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T19:14:11.323830Z",
     "start_time": "2020-02-26T19:14:11.316395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slicing rows, columns, depth\n",
    "arr[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra - Introduction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this section, we're going to take a step back to learn some of the basics of linear algebra - the math that powers most machine learning models. You may not need to know linear algebra just to call a method in scikit-learn to do some modeling, but this introduction to linear algebra should give you a much better understanding of how your models are working \"under the hood\".\n",
    "\n",
    "\n",
    "## The importance of linear algebra\n",
    "\n",
    "We're going to kick this section off by looking at some of the many places that linear algebra is used in machine learning - from deep learning through Natural Language Processing and dimensionality reduction techniques such as Principle Component Analysis.\n",
    "\n",
    "## Systems of linear equations\n",
    "\n",
    "We then start to dig into the math! We look at the idea of linear simultaneous equations - a set of two or more equations each of which is linear (can be plotted on a graph as a straight line). We then see how such equations can be represented as vectors or matrices to represent such systems efficiently.\n",
    "\n",
    "## Scalars, vectors, matrices, and tensors\n",
    "\n",
    "In a code along, we'll introduce the concepts and concrete representations (in NumPy) of scalars, vectors, matrices, and tensors - why they are important and how to create them. \n",
    "\n",
    "## Vector/matrix operations\n",
    "\n",
    "We then start to build up the basic skills required to perform matrix operations such as addition and multiplication.  You will also cover key techniques used by many machine learning models to perform their calculations covering both the Hadamard product and the (more common) dot product. \n",
    "\n",
    "\n",
    "## Solving systems of linear equations using NumPy\n",
    "\n",
    "We then bring the previous work together to look at how to use NumPy to solve systems of linear equations, introducing the identity and inverse matrices along the way.\n",
    "\n",
    "\n",
    "## Regression analysis using linear algebra and NumPy\n",
    "\n",
    "Having built up a basic mathematical and computational foundation for linear algebra, you will solve a real data problem - looking at how to use NumPy to solve a linear regression using the ordinary least squares (OLS) method.\n",
    "\n",
    "\n",
    "## Computational complexity\n",
    "\n",
    "Finally, we look at the idea of computational complexity and Big O notation, showing why OLS is computationally inefficient, and that a gradient descent algorithm can instead be used to solve a linear regression much more efficiently.\n",
    "\n",
    " \n",
    "## Summary\n",
    "\n",
    "Linear Algebra is so foundational to machine learning that you're going to see it referenced many times as the course progresses. In this section, the goal is to give you both a theoretical introduction and some computational practice, solving a real-life problem by writing the code required to solve a linear regression using OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation for Linear Algebra in Data Science\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this section, you'll learn about algebra as a foundational step for data science, and later on statistics. Linear algebra is also very important when moving on to machine learning models, where a solid understanding of linear equations plays a major role. This lesson will attempt to present some motivational examples of how and why a solid foundation of linear algebra is valuable for data scientists.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- State the importance of linear algebra in the fields of data science and machine learning \n",
    "- Describe the areas in AI and machine learning where linear algebra might be used for advanced analytics \n",
    "\n",
    "## Background \n",
    "\n",
    "\n",
    "While having a deep understanding of linear algebra may not be mandatory, some basic knowledge is undoubtedly extremely helpful in your journey towards becoming a data scientist.\n",
    "\n",
    "You may already know a number of linear algebraic concepts without even knowing it. Examples are: matrix multiplication and dot-products. Later on, you'll learn more complex algebraic concepts like the calculation of matrix determinants, cross-products, and eigenvalues/eigenvectors. As a data scientist, it is important to know some of the theories as well as having a practical understanding of these concepts in a real-world setting.\n",
    "\n",
    "## An analogy \n",
    "\n",
    "Think of a simple example where you first learn about a sine function as an infinite polynomial while learning trigonometry. Students usually practice this function by passing different values to this function and getting the expected results and then manage to relate this to triangles and vertices. When learning advanced physics, students get to learn more applications of sine and other similar functions in the area of sound and light. In the domain of Signal Processing for unidimensional data, these functions pop up again to help you solve filtering, time-series related problems. An introduction to numeric computation around sine functions can not alone help you understand its wider application areas. In fact, sine functions are everywhere in the universe from music to light/sound/radio waves, from pendulum oscillations to alternating current.\n",
    "\n",
    "## Why Linear Algebra?\n",
    "\n",
    "> Linear algebra is the branch of mathematics concerning vector spaces and linear relationships between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.\n",
    "\n",
    "Analogous to the example we saw above, it's important that a data scientist understands how data structures are built with vectors and matrices following the geometric intuitions from linear algebra, in addition to the numeric calculations. A data-focused understanding of linear algebra can help machine learning practitioners decide what tools can be applied to a given problem and how to interpret the results of experiments. You'll see that a good understanding of linear algebra is particularly useful in many ML/AI algorithms, especially in deep learning, where a lot of the operations happen under the hood.\n",
    "\n",
    "Following are some of the areas where linear algebra is commonly practiced in the domain of data science and machine learning:  \n",
    "\n",
    "### Computer Vision / Image Processing\n",
    "\n",
    "<img src=\"images/rgb.png\" width=\"600\">\n",
    "\n",
    "Computers are designed to process binary information only (only 0s and 1s). How can an image such as the dog shown here, with multiple attributes like color, be stored in a computer? This is achieved by storing the pixel intensities for red, blue and green colors in a matrix format. Color intensities can be coded into this matrix and can be processed further for analysis and other tasks. Any operation performed on this image would likely use some form of linear algebra with matrices as the back end.\n",
    "\n",
    "### Deep Learning - Tensors\n",
    "\n",
    "Deep Learning is a sub-domain of machine learning, concerned with algorithms that can imitate the functions and structure of a biological brain as a computational algorithm. These are called artificial neural networks (ANNs). \n",
    "\n",
    "The algorithms usually store and process data in the form of mathematical entities called tensors. A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector is actually such a tensor), a 2-D matrix (like a data frame), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize.\n",
    "\n",
    "\n",
    "<img src=\"images/tensor.png\" width=\"850\">\n",
    "\n",
    "\n",
    "As shown in the image above where different input features are being extracted and stored as spatial locations inside a tensor which appears as a cube. A tensor encapsulates the scalar, vector, and the matrix characteristics. For deep learning, creating and processing tensors and operations that are performed on these also require knowledge of linear algebra. Don't worry if you don't fully understand this right now, you'll learn more about tensors later!\n",
    "\n",
    "### Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) is another (very popular) area in Machine Learning dealing with text data. The most common techniques employed in NLP include BoW (Bag of Words) representation, Term Document Matrix etc. As shown in the image below, the idea is that words are being encoded as numbers and stored in a matrix format. Here, we just use 3 sentences to illustrate this:\n",
    "\n",
    "<img src=\"images/NLPmatrix.png\" width=\"650\">\n",
    "\n",
    "\n",
    "This is just a short example, but you can store long documents in (giant) matrices like this. Using these counts in a matrix form can help perform tasks like semantic analysis, language translation, language generation etc.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction techniques, which are heavily used when dealing with big datasets, use matrices to process data in order to reduce its dimensions. Principle Component Analysis (PCA) is a widely used dimensionality reduction technique that relies solely on calculating eigenvectors and eigenvalues to identify principal components as a set of highly reduced dimensions. The picture below is an example of a three-dimensional data being mapped into two dimensions using matrix manipulations. \n",
    "\n",
    "<img src=\"images/pca.png\" width = \"900\">\n",
    "\n",
    "Great, you now know about some key areas where linear algebra is used! In the following lessons, you'll go through an introductory series of lessons and labs that will cover basic ideas of linear algebra: an understanding of vectors and matrices with some basic operations that can be performed on these mathematical entities. We will implement these ideas in Python, in an attempt to give you the foundational knowledge to deal with these algebraic entities and their properties. These skills will be applied in advanced machine learning sections later in the course. \n",
    "\n",
    "## Further Reading \n",
    "\n",
    "[Youtube: Why Linear Algebra](https://www.youtube.com/watch?v=_MxCXGF9N-8)\n",
    "\n",
    "[Boost your data science skills. Learn linear algebra.](https://towardsdatascience.com/boost-your-data-sciences-skills-learn-linear-algebra-2c30fdd008cf)\n",
    "\n",
    "[Quora: Applications of Linear Algebra in Deep Learning](https://www.quora.com/What-are-the-applications-of-linear-algebra-in-machine-learning)\n",
    "\n",
    "## Summary \n",
    "\n",
    "In this lesson, you learned about some Data Science examples that heavily rely on linear algebra principles. You looked at some use cases in practical machine learning problems where linear algebra and matrix manipulation might come in handy. In the following lessons, you'll take a deeper dive into specific concepts in linear algebra, working your way towards solving a regression problem using linear algebraic operations only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems of Linear Equations \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Linear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms between them. \n",
    "The first step towards developing a good understanding of linear algebra is to get a good sense of *what linear mappings and linear equations* are, *how these relate to vectors and matrices* and *what this has to do with data analysis*. Let's try to develop a basic intuition around these ideas by first understanding what linear equations are. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to: \n",
    "\n",
    "- Describe a system of linear equations for solving analytical problems \n",
    "- Describe how matrices and vectors can be used to solve linear equations \n",
    "- Solve a system of equations using elimination and substitution \n",
    "\n",
    "\n",
    "## What are linear equations?\n",
    "\n",
    "In mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables. For example, look at the following equations: \n",
    "\n",
    "$$\n",
    "3x + 2y - z = 0 \\\\\n",
    "2x- 2y + 4z = -2 \\\\\n",
    "-x + 0.5y - z = 0\n",
    "$$\n",
    "\n",
    "This is a system of three equations in the three variables $x$, $y$, and $z$. A solution to a linear system is an assignment of values to the variables in a way that *all the equations are simultaneously satisfied*. A solution to the system above is given by:\n",
    "\n",
    "$$\n",
    "x = 1 \\\\\n",
    "y = -8/3 \\\\\n",
    "z = -7/3\n",
    "$$\n",
    "\n",
    "These values make all three equations valid. The word \"system\" indicates that the equations are to be considered collectively, rather than individually.\n",
    "\n",
    "## Solving linear equations\n",
    "\n",
    "A system of linear equations can always be expressed in a matrix form. Algebraically, both of these express the same thing. Let's work with an example to see how this works: \n",
    "\n",
    "### Example \n",
    "\n",
    "Let's say you go to a market and buy 2 apples and 1 banana. For this, you end up paying 35 pence. If you denote apples by $a$ and bananas by $b$, the relationship between items bought and the price paid can be written down as an equation - let's call it Eq. A: \n",
    "\n",
    "$2a + b = 35$  - (Eq. A)\n",
    "\n",
    "On your next trip to the market, you buy 3 apples and 4 bananas, and the cost is 65 pence. Just like above, this can be written as Eq. B:\n",
    "\n",
    "$3a + 4b = 65$ - (Eq. B)\n",
    "\n",
    "These two equations (known as a simultaneous equations) form a system that can be solved by hand for values of $a$ and $b$ i.e., price of a single apple and banana.\n",
    " \n",
    "\n",
    "Let's solve this system for individual prices using a series of eliminations and substitutions:\n",
    "\n",
    "**Step 1:** Multiply Eq. A by 4\n",
    "\n",
    "$8a + 4b = 140$ - (Eq. C)\n",
    "\n",
    "**Step 2 :** Subtract Eq. B from Eq. C\n",
    "\n",
    "$5a = 75$ which leads to $a = 15$\n",
    "\n",
    "**Step 3:** Substitute the value of $a$ in Eq. A\n",
    "\n",
    "$30 + b = 35$ which leads to $b = 5$\n",
    "\n",
    "So the price of an apple is 15 pence and the price of the banana is 5 pence. \n",
    "\n",
    "## From equations to vectors and matrices\n",
    "\n",
    "Now, as your number of shopping trips increase along with the number of items you buy at each trip, the system of equations will become more complex and solving a system for individual price may become very expensive in terms of time and effort. In these cases, you can use a computer to find the solution.\n",
    "\n",
    "The above example is a classic linear algebra problem. The numbers 2 and 1 from Eq. A and 3 and 4 from Eq. B are linear coefficients that relate input variables a and b to the known output 15 and 5.\n",
    "\n",
    "Using linear algebra, we can write this system of equations as shown below: \n",
    "\n",
    "<img src=\"images/ss.png\" width = \"320\">\n",
    "\n",
    "\n",
    "You see that in order for a computational algorithm to solve this (and other similar) problems, we need to first convert the data we have into a set of matrix and vector objects. Machine learning involves building up these objects from the given data, understanding their relationships and how to process them for a particular problem. \n",
    "\n",
    "Solving these equations requires knowledge of defining these vectors and matrices in a computational environment and of operations that can be performed on these entities to solve for unknown variables as we saw above. We'll look into how to do this in upcoming lessons. \n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, you learned how a system of linear (simultaneous) equations can be solved using elimination and substitution, and also, how to covert these problems into matrices and vectors to be processed by computational algorithms. In the next couple of lessons, we'll look at how to describe these entities in Python and NumPy and also how to perform arithmetic operations to solve these types of equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systems of Linear Equations - Lab\n",
    "\n",
    "## Introduction\n",
    "The following scenarios present problems that can be solved as a system of equations while performing substitutions and eliminations as you saw in the previous lesson.\n",
    "\n",
    "* Solve these problems by hand, showing all the steps to work out the unknown variable values \n",
    "* Verify your answers by showing the calculated values satisfy all equations\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Solve a system of equations using elimination and substitution\n",
    "\n",
    "## Exercise 1\n",
    "Jane paid 12 dollars for 4 cups of coffee and 4 cups of tea. 3 cups of coffee cost as much as 2 cups of tea. What would be the total cost of 5 cups of coffee and 5 cups of tea?\n",
    "\n",
    "### Solution\n",
    "\n",
    "> Let $x$ be the unit price of coffee and $y$ be the unit price of tea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here \n",
    "# Answer: 5 cups of tea and 5 cups of coffee = 15 dollars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Jim has more money than Bob. If Jim gave Bob 20 dollars, they would have the same amount. If Bob gave Jim 22 dollars, however, Jim would then have twice as much as Bob. \n",
    "\n",
    "How much does each one actually have?\n",
    "\n",
    "### Solution\n",
    "> Let x be the amount of money that Jim has and y be the amount that Bob has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here \n",
    "# Answer:\n",
    "# y = 106 (Bob's amount)\n",
    "# x = 146 (Jim's amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Mia has 30 coins, consisting of quarters (25 cents) and dimes (10 cents), which totals to the amount 5.70 dollars.  \n",
    "How many of each does she have?\n",
    "\n",
    "### Solution\n",
    "\n",
    "> Let x be the number of quarters and y be the number of dimes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here \n",
    "# Answer:\n",
    "# x = 18 quarters\n",
    "# y = 12 dimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level up (Optional)\n",
    "For more practice with linear equations, visit the following links for more complex equations:\n",
    "\n",
    "* https://www.transum.org/software/SW/Starter_of_the_day/Students/Simultaneous_Equations.asp?Level=6\n",
    "* https://www.transum.org/software/SW/Starter_of_the_day/Students/Simultaneous_Equations.asp?Level=7\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, you learned how to solve linear equations by hand to find the coefficient values. You'll now move forward to have a deeper look into vectors and matrices and how Python and NumPy can help us solve more complex equations in an analytical context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalars, Vectors, Matrices, and Tensors - Code Along\n",
    "\n",
    "\n",
    "## Introduction \n",
    "In this lesson, you'll be introduced to the basic mathematical entities used in linear algebra. We'll also look at how these entities are created (and later manipulated) in Python using NumPy. \n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "\n",
    "- Compare scalars, vectors, matrices, and tensors \n",
    "- Create vectors and matrices using Numpy and Python\n",
    "- Use the transpose method to transpose Numpy matrices \n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "Let's start by defining some mathematical entities that data scientists routinely come across while dealing with machine learning and deep learning algorithms. These entities are used to store, process and represent our data and analytical activities are mainly focused on manipulating these algebraic entities to provide solutions to unknown data entities. \n",
    "\n",
    "<img src=\"images/new_objects.png\" width = \"600\">\n",
    "\n",
    "## Scalars\n",
    "> A scalar is a **single number** \n",
    "\n",
    "A scalar is the simplest entity in linear algebra compared to other objects, which are usually arrays of multiple numbers. In literature, you'll find scalars represented as lower case italics characters. Scalars need to be defined in terms of the type of number they carry. For example: \n",
    "\n",
    "* **Real valued scalars**: Let $S \\in  \\mathbb{R} $  be the salary of an individual\n",
    "* **Natural number scalars**: Let $n \\in \\mathbb{N}$ be the number of floors in a building\n",
    "\n",
    "\n",
    "## Vectors \n",
    "\n",
    "> A vector is an **array** of numbers arranged in some order, as opposed to the single numbered scalar. \n",
    "\n",
    "The numbers contained within a vector are known as scalar components of the vector. Vectors are built from individual components, which are numerical in nature. We can think of a vector as a list of numbers, and vector algebra as operations performed on the numbers in the list. \n",
    "\n",
    "\\begin{equation}\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "  x_{1} \\\\\n",
    "  x_{2} \\\\\n",
    "  \\vdots \\\\\n",
    "  x_{n-1} \\\\\n",
    "  x_{n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Where $x$ is the name of the vector and $(x_1,x_2, \\ldots, x_{n-1}, x_n)$ are the scalar components of the vector.\n",
    "\n",
    "In machine learning systems like regression you saw earlier, the output variable is known as a **target vector** with the lowercase $y$ when describing the training of a machine learning algorithm.\n",
    "\n",
    "We can set index values to elements of a vector by defining a set containing the indices and write the set as a subscript. For example, to access $x_1, x_3$ and $x_6$, we define the set $S=\\{1,3,6\\}$, and call it $x_S$. \n",
    "\n",
    "\n",
    "### A geometric intuition\n",
    "\n",
    "A vector can be thought of as an entity that represents spatial coordinates in an n-dimensional space, where n is the number of dimensions. A vector can also represent a line from the origin of the vector space with a direction and a magnitude, based on scalar components. Below is an example of a vector in 3D vector space:  \n",
    "\n",
    "![](./images/vec2.png)\n",
    "\n",
    "Let’s look at how to define a vector in Python.\n",
    "\n",
    "### Defining a vector in Python\n",
    "\n",
    "In Python, one of the easiest ways to represent a vector is by using Numpy arrays. The list scalar values can be used to create a vector in Python as shown below:\n",
    "\n",
    "```python \n",
    "# create a vector from list [2,4,6]\n",
    "import numpy as np\n",
    "v = np.array([2, 4, 6])\n",
    "print(v)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:43:02.687928Z",
     "start_time": "2020-02-26T17:43:02.257668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "v = np.array([2, 4, 6])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing a vector\n",
    "\n",
    "There will be times when you have a lot of data in a vector (or array as we now know it) and you want to extract a portion of the data for some analysis. For example, maybe you want to know the first few values of a long array, or you want the integral of data between $x = 4$ and $x = 6$, but your vector covers $0 < x < 10$. Indexing is the way to do these things. Let's generate a long vector to see this action using 10 values between -pi and pi (i.e. -3.14 to 3.14):\n",
    "\n",
    "```python\n",
    "x = np.linspace(-np.pi, np.pi, 10)\n",
    "print(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:43:09.891037Z",
     "start_time": "2020-02-26T17:43:09.875443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.14159265 -2.44346095 -1.74532925 -1.04719755 -0.34906585  0.34906585\n",
      "  1.04719755  1.74532925  2.44346095  3.14159265]\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(-np.pi, np.pi, 10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the index values to address individual scalar values within this vector, similar to Python list indexing as shown below:\n",
    "```python\n",
    "print (x[0])  # first element\n",
    "print (x[2])  # third element\n",
    "print (x[-1]) # last element\n",
    "print (x[-2]) # second to last element\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:43:14.472563Z",
     "start_time": "2020-02-26T17:43:14.463833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.141592653589793\n",
      "-1.7453292519943295\n",
      "3.141592653589793\n",
      "2.443460952792061\n"
     ]
    }
   ],
   "source": [
    "print (x[0])  # first element\n",
    "print (x[2])  # third element\n",
    "print (x[-1]) # last element\n",
    "print (x[-2]) # second to last element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select a range of elements too. The syntax a:b extracts the a-th to (b-1)-th elements. The syntax a:b:n starts at a, skips n elements up to the index b.\n",
    "```python \n",
    "print (x[1:4])     # second to fourth element. Element 5 is not included\n",
    "print (x[0:-1:2])  # every other element\n",
    "print (x[:])       # print the whole vector\n",
    "print (x[-1:0:-1]) # reverse the vector!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:43:19.364513Z",
     "start_time": "2020-02-26T17:43:19.354510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.44346095 -1.74532925 -1.04719755]\n",
      "[-3.14159265 -1.74532925 -0.34906585  1.04719755  2.44346095]\n",
      "[-3.14159265 -2.44346095 -1.74532925 -1.04719755 -0.34906585  0.34906585\n",
      "  1.04719755  1.74532925  2.44346095  3.14159265]\n",
      "[ 3.14159265  2.44346095  1.74532925  1.04719755  0.34906585 -0.34906585\n",
      " -1.04719755 -1.74532925 -2.44346095]\n"
     ]
    }
   ],
   "source": [
    "print (x[1:4])     # second to fourth element. Element 5 is not included\n",
    "print (x[0:-1:2])  # every other element\n",
    "print (x[:])       # print the whole vector\n",
    "print (x[-1:0:-1]) # reverse the vector!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "> A matrix is a 2-dimensional array of numbers written between square brackets. \n",
    "\n",
    "As compared to vectors, a matrix is a multi-dimensional array of scalars that can possibly have multiple rows as well as columns. It is often denoted by an $m \\times n$ notation where $m$ is the number of rows and $n$ is number of columns, as shown below. Every scalar component of a matrix can be addressed by specifying (row, column) values as tuples $(m, n)$. A matrix is usually written down as:\n",
    "\n",
    "\n",
    "$$\n",
    "   A=\n",
    "  \\left[ {\\begin{array}{cccc}\n",
    "   A_{1,1} & A_{1,2} & \\ldots &A_{1,n} \\\\\n",
    "   A_{2,1} & A_{2,2} & \\ldots &A_{2,n} \\\\\n",
    "   \\vdots& \\vdots & \\ddots & \\vdots \\\\\n",
    "   A_{m,1} & A_{m,2} & \\ldots &A_{m,n} \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "We usually give matrices uppercase variable names with bold typeface, such as $A$. If a real-valued matrix $A$ has a height of $m$ and a width of $n$ as above, we state this as $A \\in \\mathbb{R}^{m \\times n}$. In machine learning, a vector can be seen as a special case of a matrix.\n",
    "\n",
    "> A vector is a matrix that has only 1 column, so you have an $(m \\times 1)$-matrix. $m$ is the number of rows, and 1 here is the number of columns, so a matrix with just one column is a vector.\n",
    "\n",
    "### Defining a matrix in Python\n",
    "\n",
    "As opposed to one-dimensional arrays used by vectors, we can represent a matrix in Python using a multi-dimensional NumPy array. A NumPy array can be constructed given a list of lists. For example, below is a 3 row, 3 column matrix created from a list of three lists.\n",
    "```python\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:43:51.863900Z",
     "start_time": "2020-02-26T17:43:51.844253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(X)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: Take special care with brackets during definition as opening and closing of the square brackets signifies a new row.\n",
    "\n",
    "\n",
    "\n",
    "We can also define matlab styles matrices (for those used to matlab definitions) in the following way:\n",
    "```python\n",
    "Y = np.mat([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(Y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:43:55.721037Z",
     "start_time": "2020-02-26T17:43:55.710250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.matrix"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.mat([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(Y)\n",
    "type(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy **matrices** are always 2-dimensional, while numpy **arrays** (also referred to as ndarrays) are N-dimensional. Matrix objects are a subclass of ndarray, so they inherit all the attributes and methods of ndarrays. For multidimensional arrays/matrices with more than 2 dimensions, it is always best to use arrays. Arrays are the standard vector/matrix/tensor type of NumPy, and most NumPy functions return arrays and not matrices.\n",
    "\n",
    "Arrays also offer a clear distinction between element-wise operations and linear algebra operations as you'll see later.\n",
    "\n",
    "### Matrix indexing\n",
    "\n",
    "In 2D-arrays like the one we created above you can use a (row, column) notation. Use a `:` to indicate all rows or all columns. Remember that the indexing for both vectors and matrices start with 0 and ends at (m-1) and (n-1).\n",
    "\n",
    "```python\n",
    "print (X[0, 0]) # element at first row and first column\n",
    "print (X[-1, -1]) # element from the last row and last column \n",
    "print (X[0, :]) # first row and all columns\n",
    "print (X[:, 0]) # all rows and first column \n",
    "print (X[:]) # all rows and all columns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:44:09.160388Z",
     "start_time": "2020-02-26T17:44:09.149098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "9\n",
      "[1 2 3]\n",
      "[1 4 7]\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "print (X[0, 0]) # element at first row and first column\n",
    "print (X[-1, -1]) # element from the last row and last column \n",
    "print (X[0, :]) # first row and all columns\n",
    "print (X[:, 0]) # all rows and first column \n",
    "print (X[:]) # all rows and all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use indexing to address and assign new values to elements of a matrix:\n",
    "```python\n",
    "X[:, 0] = [11, 12, 13]  # set the values in first column to this sequence\n",
    "X[2, 2] = 15  # set a single element in third row and third column\n",
    "print (X)\n",
    "\n",
    "X[2] = 16  # sets everything in the third row to 16!\n",
    "print (X)\n",
    "\n",
    "X[:,2] = 17  # sets everything in the third column to 17!\n",
    "print (X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:44:23.306305Z",
     "start_time": "2020-02-26T17:44:23.295119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  2  3]\n",
      " [12  5  6]\n",
      " [13  8 15]]\n",
      "[[11  2  3]\n",
      " [12  5  6]\n",
      " [16 16 16]]\n",
      "[[11  2 17]\n",
      " [12  5 17]\n",
      " [16 16 17]]\n"
     ]
    }
   ],
   "source": [
    "X[:, 0] = [11, 12, 13]  # set the values in first column to this sequence\n",
    "X[2, 2] = 15  # set a single element in third row and third column\n",
    "print (X)\n",
    "\n",
    "X[2] = 16  # sets everything in the third row to 16!\n",
    "print (X)\n",
    "\n",
    "X[:,2] = 17  # sets everything in the third column to 17!\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of an array\n",
    "\n",
    "The shape (or \"dimensions\") of a vector/matrix array tells us the number of values for each dimension. For a 2-dimensional array it gives you the number of rows and the number of columns. Let’s find the shape of our preceding 2-D and 3-D arrays created above. For a NumPy object, you can access its shape as shown below:\n",
    "```python\n",
    "print(x.shape) # the vector with 10 scalars\n",
    "print (X.shape) # the 2-D Matrix with 3X3 scalar components\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:44:36.086489Z",
     "start_time": "2020-02-26T17:44:36.078200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape) # the vector with 10 scalars\n",
    "print (X.shape) # the 2-D Matrix with 3X3 scalar components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector has only one dimension as shown by the shape parameter whereas the 2D-matrix has 3 rows and 3 columns\n",
    "\n",
    "## Transposition \n",
    "\n",
    "Using transposition, you can convert a row vector to a column vector and vice versa. Let's see how its done in vectors and matrices.\n",
    "\n",
    "<img src=\"images/new_vector.png\" width=\"150\">\n",
    "\n",
    "Neural networks frequently process weights and inputs of different sizes where the dimensions do not meet the requirements of matrix multiplication. Matrix transpose provides a way to \"rotate\" one of the matrices so that the operation complies with multiplication requirements and can continue. There are two steps to transpose a matrix:\n",
    "\n",
    "* Rotate the matrix right 90° clockwise.\n",
    "* Reverse the order of elements in each row (e.g. [a b c] becomes [c b a]).\n",
    "This can be better understood looking at this image :\n",
    "\n",
    "<img src=\"images/new_matrix.png\" width=\"350\">\n",
    "\n",
    "Numpy provides the transpose operation by using the `.T` attribute or the `np.transpose()` function with the array that needs to be transposed as shown below:\n",
    "\n",
    "```python\n",
    "# create a transpose of a matrix\n",
    "\n",
    "A = np.array([\n",
    "   [1, 2, 3], \n",
    "   [4, 5, 6],\n",
    "   [7, 8, 9]])\n",
    "\n",
    "A_transposed = A.T\n",
    "A_transposed_2 = np.transpose(A)\n",
    "\n",
    "print(A,'\\n\\n', A_transposed, '\\n\\n', A_transposed_2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:44:47.858447Z",
     "start_time": "2020-02-26T17:44:47.852075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]] \n",
      "\n",
      " [[1 4 7]\n",
      " [2 5 8]\n",
      " [3 6 9]] \n",
      "\n",
      " [[1 4 7]\n",
      " [2 5 8]\n",
      " [3 6 9]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "   [1, 2, 3], \n",
    "   [4, 5, 6],\n",
    "   [7, 8, 9]])\n",
    "\n",
    "A_transposed = A.T\n",
    "A_transposed_2 = np.transpose(A)\n",
    "\n",
    "print(A,'\\n\\n', A_transposed, '\\n\\n', A_transposed_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "In some cases, you'll need an array with more than two axes. In the general case:\n",
    ">An array of numbers arranged on a regular grid with a variable number of axes is known as a tensor. \n",
    "\n",
    "A vector is a one-dimensional or \"first order tensor\" and a matrix is a two-dimensional or \"second order tensor\".\n",
    "Tensor notation is just like matrix notation, with a capital letter that represents a tensor, and lowercase letters with a subscript representing scalar values within the tensor. Many operations that can be performed with scalars, vectors, and matrices can be reformulated to be performed with tensors as well. The image below shows some of these operations for a  3D tensor. \n",
    "\n",
    "<img src=\"images/new_tensors.png\" width=\"700\">\n",
    "\n",
    "As a tool, tensors and tensor algebra are widely used in the fields of physics and engineering, and in data science it is particularly useful when you'll learn about deep learning models. \n",
    "We'll revisit tensors and relevant operations in the deep learning sections and explain how tensors are created, manipulated, and saved using more advanced analytical tools like Keras. \n",
    "\n",
    "## Summary \n",
    "\n",
    "In this lesson, you learned about basic mathematical entities including scalars, vectors, matrices, and tensors to solve linear algebraic problems. You focused on creating vectors and matrices in Python and Numpy. You also saw how to index and slice these entities and check for the shape of underlying data elements. Next, you'll look at some of the key operations with matrices and vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication - Code Along\n",
    "\n",
    "## Introduction\n",
    "Understanding matrix operations is very important for a deeper understanding of linear algebra. We know matrices are used throughout the field of machine learning in the description of algorithms and representation of data. In this lesson, we shall discover how to manipulate matrices in Python and Numpy.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Compute the dot product for matrices and vectors \n",
    "- Calculate a cross product using Numpy \n",
    "- Define a cross product\n",
    "\n",
    "## Definition\n",
    "\n",
    "Multiplication of two matrices is one of the most crucial operations involving matrices. You can write the matrix product just by placing two or more matrices together, for example, \n",
    "\n",
    "> $C = AB$\n",
    "\n",
    "The standard product of two matrices is not just a matrix containing the element-wise product of the individual elements. This type of operation is a _special case_ and is called the element-wise product, or the **Hadamard product**.\n",
    "\n",
    "## Hadamard product\n",
    "\n",
    "Two matrices with the same dimensions can be multiplied together. Such element-wise matrix multiplication is called the Hadamard product. It's not the typical operation meant when referring to matrix multiplication, therefore a different operator is often used, such as a circle $\\circ$. \n",
    "> $C = A \\circ B$\n",
    "\n",
    "As with element-wise addition and subtraction, element-wise multiplication involves the multiplication of elements from each parent matrix to calculate the values in the new matrix as shown below.\n",
    "\n",
    "$$ A \\circ B = \n",
    "   \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1} * B_{1,1} & A_{1,2} * B_{1,2}\\\\\n",
    "   A_{2,1} * B_{2,1}& A_{2,2} * B_{2,2} \\\\\n",
    "   A_{3,1} * B_{3,1} & A_{3,2} * B_{3,2} \\\\\n",
    "  \\end{array} } \\right] \n",
    "$$\n",
    "\n",
    "The Hadamard product can be calculated in Python using the $*$ operator between two NumPy arrays: \n",
    "\n",
    "```python\n",
    "# Element-wise Hadamard product\n",
    "import numpy as np\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(A)\n",
    "B = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(B)\n",
    "print ('\\nHadamard product\\n\\n', A * B)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:52:41.105617Z",
     "start_time": "2020-02-26T17:52:41.093987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Hadamard product\n",
      "\n",
      " [[ 1  4  9]\n",
      " [16 25 36]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(A)\n",
    "B = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(B)\n",
    "print ('\\nHadamard product\\n', A * B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "\n",
    "The matrix dot product is more complicated than the previous operations and involves a rule as **not all matrices can be dot multiplied together**. The rule is as follows:\n",
    "\n",
    "The matrix product of matrices $A$ and $B$ is a another matrix $C$. For defining this product, $A$ must have the same number of dimensions as $B$ has rows. \n",
    "\n",
    "> When using the dot product, the number of columns in the first matrix must be equal the number of rows in the second matrix \n",
    "\n",
    "For example, think of a matrix $A$ having $m$ rows and $n$ columns and matrix $B$ having $n$ rows and $k$ columns. Provided the $n$ columns in $A$ and $n$ rows $b$ are equal, the result is a new matrix with $m$ rows and $k$ columns. The dot product can be shown using (.) or (dot). \n",
    "\n",
    "> $ C_{(m, k)} = A_{(m, n)} \\cdot B_{(n, k)}$ OR $ C_{(m, k)} = A_{(m, n)} \\text{  dot  } B_{(n, k)}$\n",
    "\n",
    "The product operation is deﬁned by\n",
    "\n",
    "$$ \\large C_{i, j}= \\sum_k A_{i, k}B_{k, j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The calculations are performed as follows:\n",
    "\n",
    "\n",
    "$$ A = \n",
    "   \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1}& A_{1,2} \\\\\n",
    "   A_{2,1}& A_{2,2}  \\\\\n",
    "   A_{3,1} & A_{3,2} \\\\\n",
    "  \\end{array} } \\right] \n",
    "$$\n",
    "\n",
    "$$ B = \n",
    "   \\left[ {\\begin{array}{cc}\n",
    "   B_{1,1}&  B_{1,2} \\\\\n",
    "   B_{2,1} & B_{2,2} \\\\\n",
    "  \\end{array} } \\right] \n",
    "$$\n",
    "\n",
    "$$ C = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1}* B_{1,1}+ A_{1,2}*B_{2,1} & A_{1,1}* B_{1,2}+ A_{1,2}*B_{2,2} \\\\\n",
    "   A_{2,1}* B_{1,1}+ A_{2,2}*B_{2,1} & A_{2,1}* B_{1,2}+ A_{2,2}*B_{2,2} \\\\\n",
    "   A_{3,1}* B_{1,1}+ A_{3,2}*B_{2,1} & A_{3,1}* B_{1,2}+ A_{3,2}*B_{2,2} \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "This rule applies for a chain of matrix multiplications.  The number of columns in one matrix in the chain must match the number of rows in the following matrix in the chain. The intuition for the matrix multiplication is that you calculate the dot product between each row in matrix $A$ with each column in matrix $B$. For example, you can step down rows of column $A$ and multiply each with column 1 in $B$ to give the scalar values in column 1 of $C$.\n",
    "\n",
    "This is made clear with the following worked example between two matrices.\n",
    "\n",
    "\n",
    "$$\n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "   1 & 2 & 3 \\\\\n",
    "   4 & 5 & 6  \\\\\n",
    "   7 & 8 & 9 \\\\\n",
    "   10 & 11 & 12 \\\\\n",
    "  \\end{array} } \\right] \\times\n",
    "    \\left[ {\\begin{array}{cc}\n",
    "   2 & 7 \\\\\n",
    "   1 & 2 \\\\\n",
    "   3 & 6 \\\\\n",
    "  \\end{array} } \\right] =\n",
    "   \\left[ {\\begin{array}{cc}\n",
    "   2*1 + 1*2 + 3*3  & 7*1+2*2+6*3\\\\\n",
    "   2*4+1*5+3*6 & 7*4+2*5+6*6 \\\\\n",
    "   2*7+1*8+3*9 & 7*7+2*8+6*9 \\\\\n",
    "   2*10+1*11+3*12 & 7*10+2*11+6*12 \\\\\n",
    "  \\end{array} } \\right] =\n",
    "    \\left[ {\\begin{array}{cc}\n",
    "   13 & 29 \\\\\n",
    "   31 & 74  \\\\\n",
    "   49 & 119 \\\\\n",
    "   67 & 164 \\\\\n",
    "  \\end{array} } \\right] \n",
    "$$\n",
    "\n",
    "\n",
    "Let's define above matrices and see how to achieve this in Python and Numpy using the `.dot()` method :\n",
    "\n",
    "```python\n",
    "# matrix dot product\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "B = np.array([[2, 7], [1, 2], [3, 6]])\n",
    "\n",
    "C = A.dot(B)\n",
    "\n",
    "print(A, '\\ndot', '\\n', B, '\\n = \\n', C)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:55:47.877653Z",
     "start_time": "2020-02-26T17:55:47.861287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "dot \n",
      " [[2 7]\n",
      " [1 2]\n",
      " [3 6]] \n",
      " = \n",
      " [[ 13  29]\n",
      " [ 31  74]\n",
      " [ 49 119]\n",
      " [ 67 164]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "B = np.array([[2, 7], [1, 2], [3, 6]])\n",
    "\n",
    "C = A.dot(B)\n",
    "\n",
    "print(A, '\\ndot', '\\n', B, '\\n = \\n', C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-vector dot product\n",
    "\n",
    "A matrix and a vector can be multiplied together as long as the rule of matrix multiplication (stated above) is observed. The number of columns in the matrix must equal the number of rows in the vector. As with matrix multiplication, the operation can be written using the dot notation. Because the vector only has one column, the result is always a vector. See the general approach below where A is the matrix being multiplied to v, a vector: \n",
    "\n",
    "\n",
    "\n",
    "$$ A = \n",
    "   \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1}& A_{1,2} \\\\\n",
    "   A_{2,1}& A_{2,2}  \\\\\n",
    "   A_{3,1} & A_{3,2} \\\\\n",
    "  \\end{array} } \\right] \n",
    "$$\n",
    "\n",
    "$$ v = \n",
    "   \\left[ {\\begin{array}{c}\n",
    "   v_{1}\\\\\n",
    "   v_{2}\\\\\n",
    "  \\end{array} } \\right] \n",
    "$$\n",
    "\n",
    "$$\n",
    "C = \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1} * v_1 + A_{1,2} * v_2 \\\\\n",
    "   A_{2,1} * v_1 + A_{2,2} * v_2 \\\\\n",
    "   A_{3,1} * v_1 + A_{3,2} * v_2 \\\\\n",
    "  \\end{array} } \\right]\n",
    "  $$\n",
    "\n",
    "The matrix-vector multiplication can be implemented in NumPy using the `.dot()` method as seen before: \n",
    "\n",
    "```python\n",
    "# matrix-vector multiplication\n",
    "\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "v = np.array([0.5, 0.5])\n",
    "\n",
    "C = A.dot(v)\n",
    "\n",
    "print(A, 'dot', v, f'= {C}', sep='\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:58:04.597230Z",
     "start_time": "2020-02-26T17:58:04.571800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "dot\n",
      "[0.5 0.5]\n",
      "= [1.5 3.5 5.5]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "v = np.array([0.5, 0.5])\n",
    "\n",
    "C = A.dot(v)\n",
    "\n",
    "print(A, 'dot', v, f'= {C}', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross product\n",
    "\n",
    "From basic geometry, you know that a vector has magnitude (how long it is) and direction. Two vectors can be multiplied using the \"cross product\". The cross product or vector product is a binary operation on two vectors in three-dimensional space. The result is a vector which is perpendicular to the vectors being multiplied and normal to the plane containing them. \n",
    "\n",
    "<img src=\"images/new_cross.png\" width=\"200\">\n",
    "\n",
    "The cross product of two vectors a and b is denoted by $(a \\times b)$. \n",
    "\n",
    "It's defined as: \n",
    "\n",
    "> $a \\times b = \\mid a \\mid  \\mid b \\mid \\sin(\\theta) n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\mid a \\mid$  is the magnitude (length) of vector $a$\n",
    "* $\\mid b \\mid$  is the magnitude (length) of vector $b$\n",
    "* $\\theta$ is the angle between $a$ and $b$\n",
    "* $n$ is the unit vector at right angles to both $a$ and $b$\n",
    "\n",
    "If either of the vectors being multiplied is zero or the vectors are parallel then their cross product is zero. More generally, the magnitude of the product equals the area of a parallelogram with the vectors as sides. If the vectors are perpendicular the parallelogram is a rectangle and the magnitude of the product is the product of their lengths. \n",
    "\n",
    "In Numpy, you can take a cross product with `np.cross()` function: \n",
    "```python\n",
    "# Cross product between two vectors\n",
    "x = np.array([0, 0, 1])\n",
    "y = np.array([0, 1, 0])\n",
    "\n",
    "print(np.cross(x, y))\n",
    "print(np.cross(y, x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:02:11.270386Z",
     "start_time": "2020-02-26T18:02:11.263042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  0]\n",
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 0, 1])\n",
    "y = np.array([0, 1, 0])\n",
    "\n",
    "print(np.cross(x, y))\n",
    "print(np.cross(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll look at the applications of the cross product later, when studying machine learning algorithms and developing geometric intuitions. In Natural Language Processing, cross products are also important to bring text into vector space, and check for document similarity. For now we'll look a bit more into the dot product.\n",
    "\n",
    "## Summary \n",
    "\n",
    "In this lesson, you learned about matrix multiplication using the Hadamard product and the dot product. You also looked at how to take the dot product between vectors and matrices while observing the size assumptions. You also saw how cross products work between two vectors. Next, you'll learn about some properties of dot products and what makes them so special. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Systems of Linear Equations with NumPy - Code Along\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson, you'll learn how to solve a system of linear equations using matrix algebra and Numpy.  You'll also learn about the identity matrix and inverse matrices, which have some unique properties that can be used to solve for unknown values in systems of linear equations. You'll also learn how to create these using Numpy. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Define the identity matrix and its dot product \n",
    "- Define the inverse of a matrix \n",
    "- Calculate the inverse of a matrix in order to solve linear problems \n",
    "- Use matrix algebra and Numpy to solve a system of linear equations given a real-life example \n",
    "\n",
    "\n",
    "\n",
    "## Identity matrix\n",
    "\n",
    "An identity matrix is a matrix whose dot product with another matrix $M$ equals the same matrix $M$.\n",
    "\n",
    "The identity matrix is a square matrix which contains **1**s along the major diagonal (from the top left to the bottom right), while all its other entries are **0**s. The main diagonal is highlighted in the image below: \n",
    "\n",
    "<img src=\"images/diagonal.png\" width=\"250\">\n",
    "\n",
    "An identity matrix with the same $(3 \\times 3)$-shape contains all **1**s along this diagonal and **0**s everywhere else as shown below:\n",
    "\n",
    "$$\n",
    "  \\left[ {\\begin{array}{ccc}\n",
    "   1 & 0 & 0 \\\\\n",
    "   0 & 1 & 0 \\\\\n",
    "   0 & 0 & 1 \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "This would be called a $(3 \\times 3)$ identity matrix. The $(n \\times n)$ identity matrix is usually denoted by $I_n$ which is a matrix with $n$ rows and $n$ columns. \n",
    "\n",
    "The identity matrix is also called the *unit matrix* or *elementary matrix*.\n",
    "\n",
    "\n",
    "### Dot product of a matrix and its identity matrix\n",
    "\n",
    "Let's try to multiply a matrix with its identity matrix and check the output. Let's start with the coefficient matrix from the previous problem:\n",
    "\n",
    "$$\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 2  \\\\\n",
    "   3 & 4  \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "The identity matrix for this matrix would look like:\n",
    "\n",
    "$$\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & 0  \\\\\n",
    "   0 & 1  \\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "The dot product for these two matrices can be calculated as:\n",
    "```python\n",
    "import numpy as np\n",
    "A = np.array([[2,1],[3,4]])\n",
    "I = np.array([[1,0],[0,1]])\n",
    "print(I.dot(A))\n",
    "print('\\n', A.dot(I))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:13:49.381916Z",
     "start_time": "2020-02-26T18:13:49.373663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [3 4]]\n",
      "\n",
      " [[2 1]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[2,1],[3,4]])\n",
    "I = np.array([[1,0],[0,1]])\n",
    "print(I.dot(A))\n",
    "print('\\n', A.dot(I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the dot product of any matrix and the appropriate identity matrix is always the original matrix, regardless of the order in which the multiplication was performed! In other words, \n",
    "\n",
    "> $ A \\cdot I = I \\cdot A = A $\n",
    "\n",
    "NumPy comes with a built-in function `np.identity()` to create an identity matrix. Just pass in the dimension (number of rows or columns) as the argument. You can add an argument `dtype=int` to make sure the elements are integers (if not, your identity matrix will contain floats):\n",
    "```python\n",
    "print(np.identity(4, dtype=int))\n",
    "print(np.identity(5, dtype=int))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:14:15.254664Z",
     "start_time": "2020-02-26T18:14:15.241300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.identity(4, dtype=int))\n",
    "print(np.identity(5, dtype=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse matrix\n",
    "\n",
    "The *inverse* of a square matrix *A*, sometimes called a *reciprocal matrix*, is a matrix $A^{-1}$such that\n",
    "\n",
    "> $A \\cdot A^{-1} = I$\n",
    "\n",
    "where $I$ is the identity matrix \n",
    "\n",
    "The inverse of a matrix is analogous to taking reciprocal of a number and multiplying by itself to get a 1, e.g. $5 * 5^{-1} = 1$. Let's see how to get inverse of a matrix in NumPy. `numpy.linalg.inv(a)` takes in a matrix *A* and calculates its inverse as shown below: \n",
    "\n",
    "```python\n",
    "A = np.array([[4, 2, 1],[4, 8, 3],[1, 1, 0]])\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(A_inv)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:15:13.977327Z",
     "start_time": "2020-02-26T18:15:13.965188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2 1]\n",
      " [4 8 3]\n",
      " [1 1 0]] \n",
      "\n",
      " [[ 0.3 -0.1  0.2]\n",
      " [-0.3  0.1  0.8]\n",
      " [ 0.4  0.2 -2.4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[4, 2, 1],[4, 8, 3],[1, 1, 0]])\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(A, '\\n\\n', A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great. So according to the principle shown above, if we multiply $A$ with $A^{-1}$, we should get an identity matrix $I$ as the output: \n",
    "\n",
    "```python\n",
    "A_product = np.dot(A, A_inv)\n",
    "A_product\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:15:45.582872Z",
     "start_time": "2020-02-26T18:15:45.575828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -2.77555756e-17,  0.00000000e+00],\n",
       "       [-1.66533454e-16,  1.00000000e+00,  0.00000000e+00],\n",
       "       [-5.55111512e-17, -1.38777878e-17,  1.00000000e+00]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_product = np.dot(A, A_inv)\n",
    "A_product#.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the expected output was an identity matrix. Although you have **1**s along the major diagonal, the float operations returned not zeros but numbers very close to zero off-diagonal. Numpy has a `np.matrix.round()` function to convert each element of the above matrix into a decimal form. \n",
    "\n",
    "```python\n",
    "np.matrix.round(A_product)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:16:14.226757Z",
     "start_time": "2020-02-26T18:16:14.214905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -0.,  0.],\n",
       "       [-0.,  1.,  0.],\n",
       "       [-0., -0.,  1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix.round(A_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks more like the identity matrix that we saw earlier. The negative signs remain after rounding off as the original small values were negative. This, however, won't affect computation in any way. \n",
    "\n",
    "## Why do we need an inverse?\n",
    "\n",
    "You need an inverse because you can't perform division operations with matrices! **There is no concept of dividing by a matrix**. However, you can multiply by an inverse, which achieves the same thing.\n",
    "\n",
    "Imagine you want to share 10 apples with 2 people.\n",
    "\n",
    "You can divide 10 by 2, or you can take the reciprocal of 2 (which is 0.5), so the answer is:\n",
    "\n",
    "$10 \\times 0.5 = 5$ - which means they get 5 apples each.\n",
    "\n",
    "We use the very same idea here and this can be used to solve a system of linear equation in the problems we saw earlier in the section where: \n",
    "\n",
    "> $A \\cdot X = B$ (remember $A$ is the matrix of coefficients, $X$ is the unknown variable and $B$ is the output)\n",
    "\n",
    "Say you want to find matrix $X$, when you already know matrices $A$ and $B$:\n",
    "\n",
    "It would've been great if you could divide both sides by $A$ to get $X = B / A$, but remember that you can't divide. You can obtain this if you multiply both sides by $A^{-1}$, as shown below:\n",
    "\n",
    "> $ A^{-1} \\cdot A \\cdot X = A^{-1} \\cdot B$\n",
    "\n",
    "From above, we that A . A<sup>-1</sup> = I, so:\n",
    "\n",
    "> $I \\cdot X = A^{-1} \\cdot B$\n",
    "\n",
    "We can remove I (because multiplying with the identity matrix doesn't change a matrix). so:\n",
    "\n",
    "> $X = A^{-1} \\cdot B$\n",
    "\n",
    "And there we have it, our answer. \n",
    "\n",
    "## Solve a system of equations with matrix algebra \n",
    "\n",
    "Now that you know everything about converting a simple real world problem into matrix format, and steps to solve the problem, let's try it out with the apples and bananas problem:\n",
    "\n",
    "Let's say you go to a market and buy 2 apples and 1 banana. For this you end up paying 35 pence. If you denote apples by $a$ and bananas by $b$, the relationship between bought items bought and price paid can be written down as:\n",
    "\n",
    "$2a + b = 35$  - (Eq. A)\n",
    "\n",
    "In your next trip to the market, you buy 3 apples and 4 bananas, and the cost is 65 pence:\n",
    "\n",
    "$3a + 4b = 65$ - (Eq. B)\n",
    "\n",
    "As seen before, this is what that looks like in matrix notation:\n",
    "\n",
    "<img src=\"images/ab.png\" width = \"280\">\n",
    "\n",
    "So first we'll need to calculate the inverse of the square matrix containing coefficient values: \n",
    "```python\n",
    "# Define A and B \n",
    "A = np.matrix([[2, 1], [3, 4]])\n",
    "B = np.matrix([35, 65])\n",
    "\n",
    "# Take the inverse of Matrix A \n",
    "A_inv = np.linalg.inv(A)\n",
    "A_inv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:25:10.638034Z",
     "start_time": "2020-02-26T18:25:10.622393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.8, -0.2],\n",
       "        [-0.6,  0.4]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define A and B \n",
    "A = np.matrix([[2, 1], [3, 4]])\n",
    "B = np.matrix([35, 65])\n",
    "\n",
    "# Take the inverse of Matrix A \n",
    "A_inv = np.linalg.inv(A)\n",
    "A_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now take a dot product of `A_inv` and `B`. Also, as you want the output in the vector format (containing one column and two rows), you would need to transpose the matrix `B` to satisfy the multiplication rule you saw previously.\n",
    "\n",
    "> **The product of an $M \\times N$ matrix and an $N \\times K$ matrix is an $M \\times K$ matrix. The new matrix takes the number of rows from the first matrix and the number of columns from the second matrix**\n",
    "\n",
    "```python\n",
    "# Check the shape of B before after transposing\n",
    "print(B.shape)\n",
    "B = B.T\n",
    "print (B.shape)\n",
    "B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:34:01.247515Z",
     "start_time": "2020-02-26T18:34:01.238829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[35],\n",
       "        [65]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(B.shape)\n",
    "B = B.T\n",
    "print (B.shape)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can easily calculate $X$ as below:\n",
    "\n",
    "```python\n",
    "X = A_inv.dot(B)\n",
    "X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:34:05.895028Z",
     "start_time": "2020-02-26T18:34:05.881708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[15.],\n",
       "        [ 5.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(A_inv.shape)\n",
    "X = A_inv.dot(B)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the prices of apples and bananas have been calculated as 15p per apple and 5p per banana, and these values satisfy both equations. Great!\n",
    "\n",
    "The dot product of $A$ and $X$ should give matrix $B$. Let's try it:\n",
    "```python\n",
    "print(A.dot(X))\n",
    "print(B)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:35:54.700810Z",
     "start_time": "2020-02-26T18:35:54.694202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35.]\n",
      " [65.]]\n",
      "[[35]\n",
      " [65]]\n"
     ]
    }
   ],
   "source": [
    "print(X.T.dot(A.T).T)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!\n",
    "\n",
    "**You can also use `numpy.linalg.solve()` to solve a system of linear equations!**\n",
    "\n",
    "Numpy has a built-in function to solve such equations as `numpy.linalg.solve(a, b)` which takes in matrices in the correct orientation, and gives the answer by calculating the inverse. Here is how to use it. \n",
    "\n",
    "```python\n",
    "# Use Numpy's built in function solve() to solve linear equations\n",
    "x = np.linalg.solve(A, B)\n",
    "x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:36:30.262729Z",
     "start_time": "2020-02-26T18:36:30.250180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[15.],\n",
       "        [ 5.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linalg.solve(A, B)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "* [Youtube: Solving System of Linear Equations using Python](https://youtu.be/AqIrdW2-K6k)\n",
    "* [Inverse of a matrix](http://www.mathwords.com/i/inverse_of_a_matrix.htm)\n",
    "* [Don't invert that matrix](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/)\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, you learned how to calculate the inverse of a matrix in order to solve a system of linear equations. You applied the skills learned on the apples and bananas problem introduced earlier. The result of the calculations helped us get unit values of variables that satisfy both equations. In the next lab, you'll go through some other similar problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Systems of Linear Equations with NumPy - Lab\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Now you've gathered all the required skills needed to solve systems of linear equations. You saw why there was a need to calculate inverses of matrices, followed by matrix multiplication to figure out the values of unknown variables. \n",
    "\n",
    "The exercises in this lab present some problems that can be converted into a system of linear equations. \n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "\n",
    "- Use matrix algebra and NumPy to solve a system of linear equations given a real-life example \n",
    "- Use NumPy's linear algebra solver to solve for systems of linear equations\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "A coffee shop is having a sale on coffee and tea. \n",
    "\n",
    "On day 1, 29 bags of coffee and 41 bags of tea were sold, for a total of 490 dollars.\n",
    "\n",
    "On day 2, they sold 23 bags of coffee and 41 bags of tea, for which customers paid a total of 448 dollars.  \n",
    "\n",
    "How much does each bag cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:40:58.241623Z",
     "start_time": "2020-02-26T18:40:58.228160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.]\n",
      " [7.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[7.],\n",
       "        [7.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and solve the relevant system of equations\n",
    "\n",
    "# Let x be the price of a bag of coffee and y be the price of a bag of tea. \n",
    "\n",
    "# 29x + 41y = 490\n",
    "\n",
    "# 23x + 41y = 448\n",
    "\n",
    "#  Create numpy matrices from above equations\n",
    "import numpy as np\n",
    "A = np.matrix([[29, 41], [23, 41]])\n",
    "B = np.matrix([[490, 448]])\n",
    "\n",
    "# Calculate inverse of A and take the dot product\n",
    "A_inv = np.linalg.inv(A)\n",
    "X = A_inv.dot(B.T)\n",
    "print(X)\n",
    "\n",
    "# Verify the answer linalg.solve()\n",
    "np.linalg.solve(A, B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe your result\n",
    "# bag of coffee = $7 , bag of tea = $7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The cost of admission to a popular music concert was 162 dollars for 12 children and 3 adults. \n",
    "\n",
    "The admission was 122 dollars for 8 children and 3 adults in the same music concert. \n",
    "\n",
    "How much was the admission for each child and adult?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:42:03.844541Z",
     "start_time": "2020-02-26T18:42:03.831251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.]\n",
      " [14.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[10.],\n",
       "        [14.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and solve the relevant system of equations\n",
    "# 12x + 3y = 162\n",
    "# 8x + 3y = 122\n",
    "# Create matrices in numpy \n",
    "A = np.matrix([[12, 3],[8, 3]])\n",
    "B = np.matrix([162, 122])\n",
    "\n",
    "# Calculate inverse of A and take the dot product\n",
    "A_inv = np.linalg.inv(A)\n",
    "X = A_inv.dot(B.T)\n",
    "print (X)\n",
    "\n",
    "# Verify the answer linalg.solve()\n",
    "np.linalg.solve(A, B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe your result\n",
    "# price per child = $10, price per adult = $14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "You want to make a soup containing tomatoes, carrots, and onions.\n",
    "\n",
    "Suppose you don't know the exact mix to put in, but you know there are 7 individual pieces of vegetables, and there are twice as many tomatoes as onions, and that the 7 pieces of vegetables cost 5.25 USD in total. \n",
    "You also know that onions cost 0.5 USD each, tomatoes cost 0.75 USD and carrots cost 1.25 USD each.\n",
    "\n",
    "Create a system of equations to find out exactly how many of each of the vegetables are in your soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:51:52.386023Z",
     "start_time": "2020-02-26T18:51:52.369739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.]\n",
      " [2.]\n",
      " [1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[4.],\n",
       "        [2.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and solve the relevant system of equations\n",
    "# t + o + c = 7\n",
    "# .75t + .5o + 1.25c = 5.25\n",
    "# t = 2o --> t - 2o + (0c) = 0\n",
    "# 1 1 1\n",
    "# .75 .5 1.25\n",
    "# 1 -2 0\n",
    "# 7 5.25 0\n",
    "\n",
    "# Create matrices in numpy \n",
    "#A = np.matrix([[1,1,1],[0.5, 0.75, 1.25], [-2,1,0]])\n",
    "A = np.matrix([[1,1,1],[0.75, 0.5, 1.25], [1,-2,0]])\n",
    "B = np.matrix([[7, 5.25, 0]])\n",
    "\n",
    "# Calculate inverse of A and take the dot product\n",
    "A_inv = np.linalg.inv(A)\n",
    "X = A_inv.dot(B.T)\n",
    "print (X)\n",
    "\n",
    "# Verify the answer linalg.solve()\n",
    "np.linalg.solve(A,B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe your result\n",
    "#t=4,o=2,c=1\n",
    "# onions = o, tomatoes = 4, carrots = 1 , needed to make the soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "A landlord owns 3 properties: a 1-bedroom, a 2-bedroom, and a 3-bedroom house. \n",
    "\n",
    "The total rent he receives is 1240 USD. \n",
    "\n",
    "He needs to make some repairs, where those repairs cost 10% of the 1-bedroom house’s rent. The 2-bedroom repairs cost 20% of the 2-bedroom rental price and 30% of the 3-bedroom house's rent for its repairs.  The total repair bill for all three houses was 276 USD. \n",
    "\n",
    "The 3-bedroom house's rent is twice the 1-bedroom house’s rent. \n",
    "\n",
    "How much is the individual rent for three houses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:59:35.679626Z",
     "start_time": "2020-02-26T18:59:35.666844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[280.]\n",
      " [400.]\n",
      " [560.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[280.],\n",
       "        [400.],\n",
       "        [560.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x + y + z = 1240\n",
    "# .1x + .2y + .3z = 276\n",
    "# z = 2x --> 2x - z - 0y = 0\n",
    "A = np.mat([[1,1,1],[.1,.2,.3],[2,0,-1]])\n",
    "B = np.mat([1240,276,0])\n",
    "A_i = np.linalg.inv(A)\n",
    "X = A_i.dot(B.T)\n",
    "print (X)\n",
    "\n",
    "np.linalg.solve(A,B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe your result\n",
    "# Rent: house1 = 280, house2 = 400, house3 = 560"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lab, you learned how to use NumPy to solve linear equations by taking inverses and matrix multiplication and also using numpy's `solve()` function. You'll now take these skills forward and see how you can define a simple regression problem using linear algebra and solve it with Numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis using Linear Algebra and NumPy - Code Along \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist's toolbox. Here, you'll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions. \n",
    "\n",
    "[You can find a deeper mathematical and geometric explanation of the topic here](http://math.mit.edu/~gs/linearalgebra/ila0403.pdf). In this lesson, we'll try to keep things more data-oriented.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to: \n",
    "\n",
    "- Apply linear algebra to fit a function to data, describing linear mappings between input and output variables\n",
    "- Indicate how linear algebra is related to regression modeling\n",
    "\n",
    "\n",
    "## Regression analysis\n",
    "\n",
    "By now, you know that the purpose of the regression process is to fit a mathematical model to a set of observed points, in order to later use that model for predicting new values e.g. predicting sales, based on historical sales figures, predicting house prices based on different features of the house, etc. \n",
    "\n",
    "Let's use a very simple toy example to understand how this works with linear algebra. Say you are collecting data on total number of sales per day for some business. Imagine you've got three data points in the format: \n",
    "\n",
    "(day, total number of sales(in hundreds)) \n",
    "\n",
    "> (1, 1) , (2, 2) , (3, 2)\n",
    "\n",
    "If we plot these points on a scatter plot with day (x-axis) vs. sales figures (y-axis), this is what we get:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([1,2,2])\n",
    "               \n",
    "plt.plot(x, y, 'o')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T20:36:53.474088Z",
     "start_time": "2020-02-26T20:36:53.265948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([1,2,2])\n",
    "               \n",
    "plt.plot(x, y, 'o')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model to data - A quick refresher\n",
    "\n",
    "The purpose of linear regression would be to fit a mathematical model (a straight line) in the parameter space  that best describes the relationship between day and sales. Simple linear regression attempts to fit a line (in a 2-dimensional space) to describe the relationship between two variables as shown in the example below:\n",
    "\n",
    "<img src =\"images/new_regr_line.png\" width=\"500\">\n",
    "\n",
    "\n",
    "Following this, if you were to identify a relationship between the day and total number of sales, the goal would be to seek a function that describes this line and allows us to linearly map input data points (day) or **independent variable** to outcome values (sales) or **dependent variable**.  If you do this, you first assume that there is an underlying relationship that maps “days” uniquely to “number of sales”, that can be written in the function form as an equation of the straight line i.e. \n",
    "\n",
    "\n",
    "$$y = mx+c$$\n",
    "\n",
    "\n",
    "where $c$ is the intercept of the line and $m$ denotes the slope, as shown below: \n",
    "\n",
    "<img src=\"images/new_regression.png\" width =\"400\">\n",
    "\n",
    "\n",
    "\n",
    "We can write the fitting function based on the above as sales being a **function** of days.\n",
    "\n",
    "$$ \\text{sales} = f(\\text{days})$$\n",
    "\n",
    "\n",
    "or, from $y= mx+c$\n",
    "\n",
    "$$\\text{sales} = \\text{days}*x + \\text{intercept} $$\n",
    ">(where **y** is the number of sales per day and **x** represents the day. **c** (intercept) and **m** (slope) are the regression coefficients we are looking for hoping that these co-efficients will linearly map **day** to the **number of sales**). \n",
    "\n",
    "So using this, we can show our three data points ((1, 1) , (2, 2) , (3, 2)) as:\n",
    "\n",
    "> $c + m*1 = 1$\n",
    "\n",
    "> $c + m*2 = 2$\n",
    "\n",
    "> $c + m*3 = 2$\n",
    "\n",
    "\n",
    "We can see that our data points do not lie on a line. The first two points make a perfect linear system. When $x = 1$, $y = 1$; and when $x = 2$, $y = 2$ i.e. we can draw a straight line passing through these points. When x = 3, b = 2, you know the three points do not lie on the same line as first two points, and our model will be an **approximation** i.e. \n",
    "> there will be some error between the straight line and the REAL relationship between these parameters. \n",
    "\n",
    "This behavior can be simulated by using NumPy's `polyfit()` function (similar to `statsmodels.ols`) to draw a regression line to the data points as shown below. [Here is the documentation for np.polyfit()](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.polyfit.html). \n",
    "\n",
    "```python\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "# Fit with polyfit function to get c(intercept) and m(slope)\n",
    "# the degree parameter = 1 to models this as a straight line\n",
    "c, m = polyfit(x, y, 1)\n",
    "\n",
    "# Plot the data points and line calculated from ployfit\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, c + (m * x), '-')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "print(c, m)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T20:38:40.392388Z",
     "start_time": "2020-02-26T20:38:40.153818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAb20lEQVR4nO3deXQV9d3H8fePECBAACWRPeCKC6JgEGURRAHXx6VapC7V6qG22tZWrVUURHAr7kXFPEKRutUKUh+7sMomm2FREARBZJdFdggQku/zxy+2CAm5SSaZ3Lmf1zkcyb3DnY/n3HzOnPn+ZsaZGSIiEv+qhB1ARESCoUIXEYkIFbqISESo0EVEIkKFLiISEVXD2nFaWpq1aNEirN2LiMSluXPnbjGz9MLeK7bQnXPNgJFAQyAfyDKzFw/b5kbggYIfdwO/MLPPjva5LVq0IDs7O4b4IiLyPefcqqLei+UI/SBwr5nNc86lAnOdc+PNbPEh26wEupjZNufcpUAW0L5MqUVEpESKLXQz2wBsKPj7LufcEqAJsPiQbWYc8k9mAU0DzikiIsUo0VDUOdcCaAPMPspmtwP/Kn0kEREpjZiHos652sAo4B4z21nENhfiC71TEe/3AfoAZGRklDisiIgULaYjdOdcMr7M3zKz0UVs0xp4HbjKzL4rbBszyzKzTDPLTE8vdEgrIiKlVGyhO+ccMAxYYmbPFbFNBjAauNnMlgUbUUREYhHLKZeOwM3AQufcgoLXHgIyAMxsKNAPqA+84vufg2aWGXxcEREpSiyrXKYDrpht7gDuCCqUiEgk5e6D2UMh43zICH5lty79FxEpb/n5sPB9GNIOJvSHZeWzEDC0S/9FRBLCqhkwti+snwcNz4Sr/g4ndC2XXanQRUTKw3crYHw/+PIjSG0MV78KrW+AKuV3YkSFLiISpL1bYcrT8OnrkFQdLnwYzr8LqtUs912r0EVEgnBwP8zJgqmDYf8uaHsLdH0IUhtUWAQVuohIWZjBFx/AhEdh+yo4qTv0GAjHnVbhUVToIiKltXo2jOsLaz+FBq3g5g/gxG6hxVGhi4iU1Nav/RH54r9D7YbwP0Pg7J9AlaRQY6nQRURitXcrTH3GnytPSoauD0KHX0G1WmEnA1ToIiLFO7jfr1qZ8kfYtwPa3ATdHobUhmEn+wEVuohIUcz8aZUJ/WHbN/78ePeB0LBV2MkKpUIXESnMmk/9wHPNbDjudLhpFJx0cdipjkqFLiJyqG3fwIQB8MVoqN0ArnzJn2IJeeAZCxW6iAhAzjaY9izMfg1cEnR5ADr8GqrXDjtZzFToIpLYDh6A7OEw5SnI2Q5n3wjd+kKdxmEnKzEVuogkJjN/46zx/fy68uO7QI9B0Kh12MlKTYUuIoln7VwY9zCsngHpp8JP/gYndwd31Gf5VHoqdBFJHNtWwcTHYNH7UCsdrnge2twCSdGowmj8X4iIHE3Odpj+HMwa6o/CO98Hne6B6qlhJwuUCl1EoisvF7L/DJOf9KtYzroBuj0CdZuEnaxcqNBFJHrMYOk//cDzu+XQorMfeDY+O+xk5UqFLiLRsm4ejHsEVk2HtFOg91/hlJ5xP/CMhQpdRKJh+xqYNBA+/yvUTIPLn4W2P/V3RUwQKnQRiW/7dsL052HWK/5US6ff+YFnjbphJ6twKnQRiU95B2HeCPj4Sdi7BVr38gPPes3CThaaYgvdOdcMGAk0BPKBLDN78bBtHPAicBmwF7jVzOYFH1ekfI2Zv47BY5eyfnsOjeulcH/PllzdJporIuKWGSwbC+MfgS3LoHlH6PE3aNI27GTFKu/vVyxH6AeBe81snnMuFZjrnBtvZosP2eZS4OSCP+2BVwv+KxI3xsxfx4OjF5KTmwfAuu05PDh6IYBKvbLY8BmM7QvfTIP6J8ENb0PLy+Ji4FkR368qxW1gZhu+P9o2s13AEuDwvV8FjDRvFlDPOdcokIQiFWTw2KX/+WX7Xk5uHoPHLg0pkfzHjnXwwZ3wWhfY+AVcOhh+OQtOvTwuyhwq5vtVonPozrkWQBtg9mFvNQHWHPLz2oLXNhz27/sAfQAyMjJKllSknK3fnlOi16UC7N8F01+AmS+D5UHHX0Pne+Ny4FkR36+YC905VxsYBdxjZjsPf7uQf2JHvGCWBWQBZGZmHvG+SJga10thXSG/XI3rpYSQJsHlHYT5I+HjJ2DPZmh1HVzUD45pHnayUquI71exp1wAnHPJ+DJ/y8xGF7LJWuDQ0XJTYH3Z44lUnPt7tiQl+YdPpUlJTuL+ni1DSpSAzGDZOBjaET76rT9PfsdEuG5YXJc5VMz3K5ZVLg4YBiwxs+eK2OxD4G7n3Lv4YegOM9tQxLYildL3gymtcgnJtwv9LW2/ngzHngA//gucdmXcnCMvTkV8v5zZ0c98OOc6AdOAhfhliwAPARkAZja0oPSHAJfgly3eZmbZR/vczMxMy84+6iYikgh2rodJj8OCtyClHnT5A2T+DKpWCztZpeScm2tmmYW9V+wRuplNp/Bz5IduY8BdpYsnIglp/26Y8RLM+BPkH4QOd/uBZ8oxYSeLW7pSVEQqVn4ezH8TPn4cdm+EM671A89jjw87WdxToYtIxVk+wd8JcdNiaHou9HoLmrULO1VkqNBFpPxt/MIX+YqJcEwLuP4NOP2qyAw8KwsVuoiUn13f+lMr89+E6nWg5xPQ7g6oWj3sZJGkQheR4B3Y44edn7wEeQeg/S/ggvug5rFhJ4s0FbqIBCc/Dxa8DZMGwe5v/WmVi/pD/RPDTpYQVOgiEowVk/x58o2LoEkm/PgNyDgv7FQJRYUuImWzaYkv8uXjoV4GXDfcL0XUwLPCqdBFpHR2bYTJT8C8kVAtFboPhPY/18AzRCp0ESmZA3v97Ww/eQEO7oNz+8AFv4da9cNOlvBU6CISm/x8+PxdmDgQdq2HU6+A7o9p4FmJqNBFpHhfT4Fxff0dERu39bezbd4h7FRyGBW6iBRt81IY3w+W/RvqZsCPhvmBZ5WYHqUgFUyFLiJH2r0ZJj8Jc0dAtVpw8QBofyck1wg7mRyFCl1E/is3B2a9AtOeh9y90O526PIA1EoLO5nEQIUuIn7gufA9P/DcuRZaXg7dB0DayWEnkxJQoYskupXT/MBzw2fQ6Cy4Zigc3znsVFIKKnSRRLXlKz/wXPpPqNMUrsmCM6/XwDOOqdBFEs2eLTD5KcgeDsk1/dOCzvslJKeEnUzKSIUukihy98HsV2Hac/72tufcCl0fhNrpYSeTgKjQRaIuPx8WjYKJA2DHGjjlEn+FZ3rLsJNJwFToIlG2agaM7Qvr50HD1nDVy3BCl7BTSTlRoYtE0ZblMKE/fPkRpDaGq4dC614aeEacCl0kSvZ8B1P/CJ++DknV4cKH4fy7oFrNsJNJBVChi0RB7j6YkwVTn4EDu6DtLdD1IUhtEHYyqUDFFrpzbjhwBbDJzFoV8n5d4E0go+DznjGzPwcdVEQKYQZfjIYJj8L21XBSd+gxEI47LexkEoJYjtBHAEOAkUW8fxew2MyudM6lA0udc2+Z2YGAMopIYVbP8gPPddnQoBXc/AGc2C3sVBKiYgvdzKY651ocbRMg1TnngNrAVuBgIOlE5Ehbv4bx/WHJh1C7oV+5clZvqJIUdjIJWRDn0IcAHwLrgVSgl5nlF7ahc64P0AcgIyMjgF2LJJC9W2HqYJjzv5BUzZ8j73C3v72tCMEUek9gAdANOBEY75ybZmY7D9/QzLKALIDMzEwLYN8i0Xdwvy/xqX+E/bugzU1wYV9IbRh2Mqlkgij024CnzMyA5c65lcCpwJwAPlskcZnB4jF+4LntG39+vMcgaHBG2Mmkkgqi0FcDFwHTnHMNgJbA1wF8rkjiWjPHDzzXzoHjToebRsFJF4edSiq5WJYtvgN0BdKcc2uB/kAygJkNBQYCI5xzCwEHPGBmW8otsUiUbV3p77nyxQdQuwFc+ZI/xaKBp8QgllUuvYt5fz3QI7BEIokoZ5u/KGhOFrgk/9i3Dr+G6rXDTiZxRFeKioTp4AHIHgZTnoac7XD2jdCtL9RpHHYyiUMqdJEwmMGS//M30Nr6NZzQ1Q88G54ZdjKJYyp0kYq2dq5/hufqmZB+Ktz4vh94Ohd2MolzKnSRirJtlR94LhoFtdLhiuehzS2QpF9DCYa+SSLlLWc7THsWZg8FVwU63wed7oHqqWEnk4hRoYuUl7xc/yDmyU/5VSxn3QDdHoG6TcJOJhGlQhcJmhl8+Q8Y3w+2roAWnf3As/HZYSeTiFOhiwRp3TwY9zCs+gTSToHef4VTemrgKRVChS4ShO1rYOJjsPA9qJkGlz8LbW/VwFMqlL5tImWxbydMfw5mvuKPwjv9Djr9FmrUCTuZJCAVukhp5OXC3BF+4Ll3C7Tu5Qee9ZqFnUwSmApdpCTMYNm//cBzyzJo3hF6/A2atA07mYgKXSRm6xf4gec306D+SXDD29DyMg08pdJQoYsUZ8damDgQPn8XUo6FSwdD5m2QlBx2MpEfUKGLFGX/Lpj+PMx82Z9q6fgb6Hwv1KgbdjKRQqnQRQ6XdxDmvQGTn4Q9m6HVdXBRPzimedjJRI5KhS7yPTP4ajyMfwQ2fwkZ5/sLg5qeE3YykZio0EUANnzuB54rp8CxJ0CvN+HUKzTwlLiiQpfEtnM9TBoEC96GlHpwydOQ+TOoWi3sZCIlpkKXxLR/N3zyIsz4E1gedLjbDzxTjgk7mUipqdAlseTnwfy/wKTHYc8mOONaP/A89viwk4mUmQpdEsdXE/zAc9NiaNbeXxjUrF3YqUQCo0KX6Pt2kS/yFZPgmBZw/Rtw+lUaeErkqNAlunZugI8fhwVvQfU60PMJaHcHVK0edjKRcqFCl+g5sMcPOz950d8Vsf0v4IL7oOaxYScTKVfFFrpzbjhwBbDJzFoVsU1X4AUgGdhiZl2CDCkSk/w8v/xw0iDY/a0/rXLxo35duUgCiOUIfQQwBBhZ2JvOuXrAK8AlZrbaOXdccPFEYrRiEox7BDYugiaZ8OM3IOO8sFOJVKhiC93MpjrnWhxlk58Ao81sdcH2m4KJJhKDjYv9wHP5BKiXAdcN90sRNfCUBBTEOfRTgGTn3GQgFXjRzIo6mu8D9AHIyMgIYNeSsHZt9APP+X+BaqnQYxCc20cDT0loQRR6VeAc4CIgBZjpnJtlZssO39DMsoAsgMzMTAtg35JoDuyFmUNg+guQtx/O/Tl0+b0GniIEU+hr8YPQPcAe59xU4CzgiEIXKbX8fP+AiYkDYdd6OO1KuHgA1D8x7GQilUYQhf53YIhzripQDWgPPB/A54p4X0+BcX3h24XQuC1cNwyadwg7lUilE8uyxXeArkCac24t0B+/PBEzG2pmS5xz/wY+B/KB181sUflFloSxealfufLVWKibAT8a5geeVaqEnUykUopllUvvGLYZDAwOJJHI7k3+aUFz34BqtfyplfZ3QnKNsJOJVGq6UlQqj9wc//zO6S9A7l5odzt0eQBqpYWdTCQuqNAlfPn5sPA9mPgY7FwHLS+H7gMg7eSwk4nEFRW6hGvlND/w3PAZNDobrnkNju8cdiqRuKRCl3BsXgYT+sPSf0KdpnBNFpx5vQaeImWgQpeKtWcLTH4KsodDck3/tKDzfgnJKWEnE4l7KnSpGLn7YParMO05f3vbzNugyx+gdnrYyUQiQ4Uu5Ss/HxaNgokDYMcaOOUS6P4YpLcMO5lI5KjQpfx88wmMexjWz4OGreGql+EE3SpfpLyo0CV4W5b7geeXH0FqY7h6KLTupYGnSDlToUtw9nwHU56G7GFQtQZ0exjOuwuq1Qw7mUhCUKFL2eXugzmvwdRn4cAuaPtT6PogpDYIO5lIQlGhS+mZ/XfguX01nNzDDzyPOy3sZCIJSYUupbN6FoztC+uyoUEruHkMnHhh2KlEEpoKXUrmuxUw4VFY8iHUbuhXrpzVG6okhZ1MJOGp0CU2e7fC1MEw538hqRp0fQg63O1vbysilYIKXY7u4H5f4lP/CPt3QZub4MK+kNow7GQichgVuhTODBaP8adXtn0DJ14EPQZCgzPCTiYiRVChy5HWzPEDz7Vz4Lgz4KZRcNLFYacSkWKo0OW/tq70SxC/+ABqN4D/+ROcfaMGniJxQoUukLMNpj4Dc7KgSlV/F8QOv4LqtcNOJiIloEJPZAcP+Mv0pzwNOdv90Xi3vlCncdjJRKQUVOiJyMyvIx/fH7athBO6Qo9B0PDMsJOJSBmo0BPN2mw/8FwzC9JPhRvf9wNP58JOJiJlpEJPFNu+gYmP+Xuv1EqHK16ANjdDkr4CIlGh3+aoy9kO056F2UPBJcEF90PH30D11LCTiUjAii1059xw4Apgk5m1Osp27YBZQC8zez+4iFIqebn+QcyTn/KrWM7q7e9PXrdJ2MlEpJzEcoQ+AhgCjCxqA+dcEvA0MDaYWFJqZvDlP2B8P9i6Ao6/wA88G50VdjIRKWfFFrqZTXXOtShms18Bo4B2AWSS0lo3zz/Dc9UnkHYK9P4rnNJTA0+RBFHmc+jOuSbANUA3iil051wfoA9ARkZGWXct39u+GiYOhIXvQc00uPxZaHurBp4iCSaI3/gXgAfMLM8VcyRoZllAFkBmZqYFsO/Etm8HTH8eZr7ij8I7/Q46/RZq1Ak7mYiEIIhCzwTeLSjzNOAy59xBMxsTwGdLYfJyYe4ImPwk7P0OWveCbo9AvWZhJxOREJW50M3s+O//7pwbAXykMi8nZrDs3zDuEfjuK2jeCXoOgsZtwk4mIpVALMsW3wG6AmnOubVAfyAZwMyGlms6+a/1C/zA85tpUP8kuOEdaHmpBp4i8h+xrHLpHeuHmdmtZUojR9qx1g88P38XataHy56Bc26FpOSwk4lIJaNlEJXV/l0FA8+X/amWjvdA599BjbphJxORSkqFXtnkHYR5b/iB557N0Oo6uKgfHNM87GQiUsmp0CsLM/hqnB94blkKGef7C4OanhN2MhGJEyr0ymDD537guXIKHHsC9HoTTr1CA08RKREVeph2rodJg2DB25BSDy55GjJ/BlWrhZ1MROKQCj0M+3fDJy/CjD+B5UGHu6Hzfb7URURKSYVekfLzYP5fYNLjsGcTnHEtXNwfjmkRdjIRiQAVekX5agKMfwQ2LYZm7eGGt6GZbk4pIsFRoZe3bxf5Il8xyR+JX/8GnH6VBp4iEjgVennZuQE+HgTz3/IXA/V8AtrdAVWrh51MRCJKhR60A3vgk5dgxkv+rojn/RIuuA9qHht2MhGJOBV6UPLzYMFbfuC5+1t/WuXiR/26chGRCqBCD8KKSf4Kz42LoGk7+PFIyGgfdioRSTAq9LLYuNgPPJdPgHrN4bo/wxnXaOApIqFQoZfGro3w8eN+TXn1VOgxCM7to4GniIRKhV4SB/bCzCEw/QXI2w/n/hy6/F4DTxGpFFToscjPg8/ehUkDYdcGOO1KuHgA1D8x7GQiIv+hQi/O15P9nRC/XQiN28J1w6F5h7BTiYgcQYVelE1fwvh+8NVYqJsBPxrm771SpUrYyURECqVCP9zuTf5pQXPfgGq1/KmV9ndCco2wk4mIHJUK/Xu5Of75ndNfgIM50O526PIHqFU/7GQiIjFRoefnw8L3YOJjsHMdtLwcug+AtJPDTiYiUiKJXegrp8G4vrDhM2h0NlybBS06hZ1KRKRUErPQNy/zA89l/4I6TeGaLDjzeg08RSSuJVah79niB57Zf4bkmnBRP383xOSUsJOJiJRZsYXunBsOXAFsMrNWhbx/I/BAwY+7gV+Y2WeBpiyr3ByY9SpMew5y90LmbX7gWTs97GQiIoGJ5Qh9BDAEGFnE+yuBLma2zTl3KZAFVI5bDebnw6L3/cBzxxo45VI/8ExvGXYyEZHAFVvoZjbVOdfiKO/POOTHWUDTsscKwDef+Cs818+Dhq3hqpfhhC5hpxIRKTdBn0O/HfhXUW865/oAfQAyMjIC3nWBLcthQn/48iNIbQxXD4XWvTTwFJHIC6zQnXMX4gu9yHV/ZpaFPyVDZmamBbVvAPZ8B1OehuxhULUGdHsYzrsLqtUMdDciIpVVIIXunGsNvA5cambfBfGZMcvdB3Neg6nPwoFd0PancOFDUPu4Co0hIhK2Mhe6cy4DGA3cbGbLyh4pRmawaBRMHADbV8PJPaD7Y3DcaRUWQUSkMoll2eI7QFcgzTm3FugPJAOY2VCgH1AfeMX5R68dNLPM8goMwKqZ/grPdXOhQSu4eQyceGG57lJEpLKLZZVL72LevwO4I7BExVnwDoy5E1Ib+ZUrZ/WGKkkVtnsRkcoq/q4UbXmpv8Kz/Z3+9rYiIgLEY6Gn1IPO94adQkSk0tHibBGRiFChi4hEhApdRCQiVOgiIhGhQhcRiQgVuohIRKjQRUQiQoUuIhIRKnQRkYhQoYuIRIQKXUQkIlToIiIRoUIXEYkIFbqISESo0EVEIkKFLiISESp0EZGIUKGLiESECl1EJCJU6CIiEaFCFxGJCBW6iEhEqNBFRCKi2EJ3zg13zm1yzi0q4n3nnHvJObfcOfe5c65t8DG9MfPX0fGpSRz/h3/Q8alJjJm/rrx2JSISd2I5Qh8BXHKU9y8FTi740wd4teyxjjRm/joeHL2QddtzMGDd9hweHL1QpS4iUqDYQjezqcDWo2xyFTDSvFlAPedco6ACfm/w2KXk5Ob94LWc3DwGj10a9K5EROJSEOfQmwBrDvl5bcFrR3DO9XHOZTvnsjdv3lyinazfnlOi10VEEk0Qhe4Kec0K29DMssws08wy09PTS7STxvVSSvS6iEiiCaLQ1wLNDvm5KbA+gM/9gft7tiQlOekHr6UkJ3F/z5ZB70pEJC4FUegfArcUrHY5D9hhZhsC+NwfuLpNE5689kya1EvBAU3qpfDktWdydZtCz+6IiCScqsVt4Jx7B+gKpDnn1gL9gWQAMxsK/BO4DFgO7AVuK6+wV7dpogIXESlCsYVuZr2Led+AuwJLJCIipaIrRUVEIkKFLiISESp0EZGIUKGLiESE8zPNEHbs3GZgVSn/eRqwJcA4IofTd0zKU1m+X83NrNArM0Mr9LJwzmWbWWbYOSS69B2T8lRe3y+dchERiQgVuohIRMRroWeFHUAiT98xKU/l8v2Ky3PoIiJypHg9QhcRkcOo0EVEIiKuCr24B1aLlIVzrplz7mPn3BLn3BfOud+EnUmiwzlXwzk3xzn3WcH3a0Dg+4inc+jOuQuA3fhnmLYKO49ES8GzcBuZ2TznXCowF7jazBaHHE0iwDnngFpmtts5lwxMB35T8CzmQMTVEXoMD6wWKTUz22Bm8wr+vgtYQhHPxxUpKfN2F/yYXPAn0CPquCp0kYrinGsBtAFmh5tEosQ5l+ScWwBsAsabWaDfLxW6yGGcc7WBUcA9ZrYz7DwSHWaWZ2Zn45+9fK5zLtBTxyp0kUMUnNscBbxlZqPDziPRZGbbgcnAJUF+rgpdpEDB0GoYsMTMngs7j0SLcy7dOVev4O8pwMXAl0HuI64KveCB1TOBls65tc6528POJJHSEbgZ6OacW1Dw57KwQ0lkNAI+ds59DnyKP4f+UZA7iKtliyIiUrS4OkIXEZGiqdBFRCJChS4iEhEqdBGRiFChi4hEhApdRCQiVOgiIhHx/4mVDHyDKGJ3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667 0.5000000000000001\n"
     ]
    }
   ],
   "source": [
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "# Fit with polyfit function to get c(intercept) and m(slope)\n",
    "# the degree parameter = 1 to models this as a straight line\n",
    "c, m = polyfit(x, y, 1)\n",
    "\n",
    "# Plot the data points and line calculated from ployfit\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, c + (m * x), '-')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "print(c, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers obtained here reflect the slope (0.5) and intercept values (0.66).\n",
    "\n",
    "The line drawn above using this built-in regression model clearly doesn't touch all the data points. As a result, this is an **approximation** of the function you're trying to find. Now let's see how to achieve the same functionality with matrix algebra instead of the `polyfit()` function.\n",
    "\n",
    "## Create matrices and vectors\n",
    "A linear system like the one above can be solved using linear algebra! You only need to deal with a few vectors and matrices to set this up.\n",
    "\n",
    "Recalling linear systems from the previous lessons, you have:\n",
    "\n",
    "$$ \\left[ {\\begin{array}{cc} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ \\end{array} } \\right] \\left[ {\\begin{array}{c} c \\ m \\ \\end{array} } \\right] = \\left[ {\\begin{array}{c} 1 \\ 2 \\ 2 \\ \\end{array} } \\right] $$\n",
    "\n",
    "## The intercept and error terms\n",
    "\n",
    "The column of ones in the first matrix refers to the intercept ($c$) from $mx+c$. If you don't include this constant, then the function is constrained to the origin (0,0), which would strongly limit the types of relationships the model could describe. You want to include an intercept to allow for linear models to intersect with the $y$-axis at values different from 0 (in the image shown earlier, $c$ was 2, because the straight line crossed the $y$-axis at $y$=2).\n",
    "\n",
    "In above , we are hoping that there is some linear combination of the columns of the first matrix that gives us our vector of observed values (the vector with values 1,2,2).\n",
    "\n",
    "Unfortunately, we already know that this vector does not fit our model perfectly. That means it is outside the column space of A and we can't solve that equation for the vector $x$ directly. Every line we draw will have some value of **error** $e$ associated with it. \n",
    "\n",
    "**The goal is to choose the vector $x$ for unknown variables to make $e$ as small as possible**. \n",
    "\n",
    "## Ordinary least squares \n",
    "\n",
    "A common measure to find and minimize the value of this error is called *Ordinary Least Squares*. \n",
    "\n",
    "This says that our dependent variable, is composed of a linear part and error. The linear part is composed of an intercept and independent variable(s), along with their associated raw score regression weights.\n",
    "\n",
    "In matrix terms, the same equation can be written as:\n",
    "\n",
    "$ y = \\boldsymbol{X} b + e $\n",
    "\n",
    "This says to get y (sales), multiply each $\\boldsymbol{X}$ by the appropriate vector b (unknown parameters, the vector version of $m$ and $c$), then add an error term. We create a matrix $\\boldsymbol{X}$ , which has an extra column of **1**s in it for the intercept. For each day, the **1** is used to add the intercept in the first row of the column vector $b$.\n",
    "\n",
    "Let's assume that the error is equal to zero on average and drop it to sketch a proof:\n",
    "\n",
    "$ y = \\boldsymbol{X} b$\n",
    "\n",
    "\n",
    "Now let's solve for $b$, so we need to get rid of $\\boldsymbol{X}$. First we will make X into a nice square, symmetric matrix by multiplying both sides of the equation by $\\boldsymbol{X}^T$ :\n",
    "\n",
    "$\\boldsymbol{X}^T y = \\boldsymbol{X}^T \\boldsymbol{X}b $\n",
    "\n",
    "And now we have a square matrix that with any luck has an inverse, which we will call $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}$. Multiply both sides by this inverse, and we have\n",
    "\n",
    "$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{X}b $\n",
    "\n",
    "\n",
    "It turns out that a matrix multiplied by its inverse is the identity matrix $(\\boldsymbol{X}^{-1}\\boldsymbol{X})= I$:\n",
    "\n",
    "$(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y =I b $\n",
    "\n",
    "\n",
    "And you know that $Ib= b$ So if you want to solve for $b$ (that is, remember, equivalent to finding the values $m$ and $c$ in this case), you find that:\n",
    "\n",
    "$ b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\n",
    "\n",
    "Here, we'll focus on the matrix and vector algebra perspective. With least squares regression, in order to solve for the expected value of weights, referred to as $\\hat{X}$ (\"$X$-hat\"), you need to solve the above equation. \n",
    "\n",
    "Remember all above variables represent vectors. The elements of the vector X-hat are the estimated regression coefficients $c$ and $m$ that you're looking for. They minimize the error between the model and the observed data in an elegant way that uses no calculus or complicated algebraic sums.\n",
    "\n",
    "The above description can be summarized as:\n",
    "\n",
    "**Using linear regression is just trying to solve $Xb = y$. But if any of the observed points deviate from the model, you can't find a direct solution. To find a solution, you can multiply both sides by the transpose of $X$. The transpose of $X$ times $X$ will always allow us to solve for unknown variables.**\n",
    "\n",
    "## Calculate an OLS regression line\n",
    "\n",
    "Let's use the above formula to calculate a solution for our toy problem: \n",
    "```python\n",
    "# Calculate the solution\n",
    "\n",
    "X = np.array([[1, 1],[1, 2],[1, 3]])\n",
    "y = np.array([1, 2, 2])\n",
    "Xt = X.T\n",
    "XtX = Xt.dot(X)\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "Xty = Xt.dot(y)\n",
    "x_hat = XtX_inv.dot(Xty) # the value for b shown above\n",
    "x_hat\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T20:43:54.564677Z",
     "start_time": "2020-02-26T20:43:54.552654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.5       ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 1],[1, 2],[1, 3]])\n",
    "y = np.array([1, 2, 2])\n",
    "Xt = X.T\n",
    "XtX = Xt.dot(X)\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "Xty = Xt.dot(y)\n",
    "x_hat = XtX_inv.dot(Xty) # the value for b shown above\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution gives an intercept of 0.6 and slope value 0.5. Let's see what you get if you draw a line with these values with given data: \n",
    "\n",
    "```python\n",
    "# Define data points\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([1, 2, 2])\n",
    "\n",
    "# Plot the data points and line parameters calculated above\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, x_hat[0] + (x_hat[1] * x), '-')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T20:44:14.314473Z",
     "start_time": "2020-02-26T20:44:14.126534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define data points\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([1, 2, 2])\n",
    "\n",
    "# Plot the data points and line parameters calculated above\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, x_hat[0] + (x_hat[1] * x), '-')\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it, an approximated line function! Just like the one you saw with `polyfit()`, by using simple matrix algebra. \n",
    "\n",
    "## Regression with multiple variables\n",
    "\n",
    "Above, you saw how you can draw a line on a 2D space using simple regression. If you perform a similar function with multiple variables, you can have a parameter space that is not 2D. With 3 parameters, i.e. two input and one output feature, the fitting function would not be a line, but would look like a plane:\n",
    "\n",
    "<img src=\"./images/new_LinRegresChart.png\" width=\"600\">\n",
    "\n",
    "When you have more than one input variable, each data point can be seen as a feature vector $x_i$, composed of $x_1, x_2, \\ldots , x_m$ , where $m$ is the total number of features (columns). For multiple regression, each data point can contain two or more features of the input. To represent all of the input data along with the vector of output values we set up a input matrix *X* and an output vector *y*. \n",
    "\n",
    "you can write this in general terms, as you saw earlier:\n",
    " \n",
    "> $\\boldsymbol{X} \\beta \\approx y$\n",
    "\n",
    "\n",
    "Where *X* are the input feature values, $\\beta$ represents the coefficients and *y* is the output (value to be predicted). In a simple least-squares linear regression model you are looking for a vector $\\beta$ so that the product $X \\beta$ most closely approximates the outcome vector y.\n",
    "\n",
    "For each value of input features $x_i$, we can compute a predicted outcome value as:\n",
    "\n",
    "observed data $\\rightarrow$ $y = b_0+b_1x_1+b_2x_2+ \\ldots + b_px_p+ \\epsilon $\n",
    "\n",
    "predicted data $\\rightarrow$ $\\hat y = \\hat b_0+\\hat b_1x_1+\\hat b_2x_2+ \\ldots + \\hat b_px_p $\n",
    "\n",
    "error $\\rightarrow$ $\\epsilon = y - \\hat y $\n",
    "\n",
    "Just like before,  the formula to compute the beta vector remains:\n",
    "\n",
    "$ \\large b= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y $\n",
    "\n",
    "\n",
    "So you see that the general solution involves taking a matrix transpose, the inverse, and dot multiplications on the lines of solving a linear system of equations. \n",
    "\n",
    "In the next lab, you'll use a simple dataset and with the above formulation for multivariate regression, you'll try to fit a model to the data and see how well it performs. \n",
    "\n",
    "## Further reading\n",
    "\n",
    "You're strongly advised to visit the following links to develop a strong mathematical and geometrical intuition around how least squares work. These documents will provide you with a visual intuition as well as an in-depth mathematical formulation for above equations along with their proofs. \n",
    "\n",
    "* [Quora: Why do we need an extra column of ones in regression](https://www.quora.com/Why-do-we-add-an-extra-column-in-1-matrix-to-solve-normal-equation-in-linear-regression)\n",
    "\n",
    "* [An excellent visual demonstration of oridnary least squares](http://setosa.io/ev/ordinary-least-squares-regression/)\n",
    "\n",
    "* [Simple Regression in Matrix format](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf)\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, you had a gentle introduction to how we can use linear algebra to solve regression problems. You saw a toy example in the case of simple linear regression, relating days to number of sales and calculated a function that approximates the linear mapping.\n",
    "\n",
    "You also learned about how linear regression works in the context of multiple input variables and linear algebra. In the next lab, you'll use these equations to solve a real world problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Linear Algebra - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You'll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. You'll also evaluate the model fit.\n",
    "In order to make this experiment interesting, you'll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\n",
    "  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- Use matrix algebra to calculate the parameter values of a linear regression\n",
    "\n",
    "\n",
    "First, let's import necessary libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # for reading csv file\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "The dataset you'll use for this experiment is \"**Sales Prices in the City of Windsor, Canada**\", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage), etc. and an output (dependent) variable, **price**.  You'll formulate a linear algebra problem to find linear mappings from input features using the equation provided in the previous lesson. \n",
    "\n",
    "This will allow you to find a relationship between house features and house price for the given data, allowing you to find unknown prices for houses, given the input features.  \n",
    "\n",
    "A description of the dataset and included features is available [here](https://rdrr.io/cran/Ecdat/man/Housing.html).\n",
    "\n",
    "In your repository, the dataset is available as `windsor_housing.csv`. There are 11 input features (first 11 columns):\n",
    "\n",
    "\tlotsize\tbedrooms  bathrms  stories\tdriveway  recroom\tfullbase  gashw\t airco  garagepl   prefarea\n",
    "\n",
    "and 1 output feature i.e. **price** (12th column). \n",
    "\n",
    "The focus of this lab is not really answering a preset analytical question, but to learn how you can perform a regression experiment, using mathematical manipulations - similar to the one you performed using `statsmodels`. So you won't be using any `pandas` or `statsmodels` goodness here. The key objectives here are to: \n",
    "\n",
    "- Understand regression with matrix algebra and \n",
    "- Mastery in NumPy scientific computation\n",
    "\n",
    "## Stage 1: Prepare data for modeling \n",
    "\n",
    "Let's give you a head start by importing the dataset. You'll perform the following steps to get the data ready for analysis:\n",
    "\n",
    "* Initialize an empty list `data` for loading data\n",
    "* Read the csv file containing complete (raw) `windsor_housing.csv`. [Use `csv.reader()` for loading data.](https://docs.python.org/3/library/csv.html). Store this in `data` one row at a time \n",
    "\n",
    "* Drop the first row of csv file as it contains the names of variables (header) which won't be used during analysis (keeping this will cause errors as it contains text values) \n",
    "\n",
    "* Append a column of all **1**s to the data (bias) as the first column\n",
    "\n",
    "* Convert `data` to a NumPy array and inspect first few rows \n",
    "\n",
    "> NOTE: `read.csv()` reads the csv as a text file, so you should convert the contents to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Create Empty lists for storing X and y values \n",
    "data = []\n",
    "\n",
    "# Read the data from the csv file\n",
    "with open('windsor_housing.csv') as f:\n",
    "    raw = csv.reader(f)\n",
    "    # Drop the very first line as it contains names for columns - not actual data \n",
    "    next(raw)\n",
    "    # Read one row at a time. Append one to each row\n",
    "    for row in raw:\n",
    "        ones = [1.0]\n",
    "        for r in row:\n",
    "            ones.append(float(r))\n",
    "        # Append the row to data \n",
    "        data.append(ones)\n",
    "data = np.array(data)\n",
    "data[:5,:]\n",
    "\n",
    "\n",
    "# First 5 rows of raw data \n",
    "\n",
    "# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n",
    "#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n",
    "#         4.20e+04],\n",
    "#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n",
    "#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
    "#         3.85e+04],\n",
    "#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n",
    "#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
    "#         4.95e+04],\n",
    "#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n",
    "#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
    "#         6.05e+04],\n",
    "#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n",
    "#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
    "#         6.10e+04]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform a 80/20 train-test split\n",
    "\n",
    "Explore NumPy's official documentation to manually split a dataset using a random sampling method of your choice. Some useful methods are located in the [numpy.random library](https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html).\n",
    "* Perform a **random** 80/20 split on data using a method of your choice in NumPy\n",
    "* Split the data to create `x_train`, `y_train`, `x_test`, and `y_test` arrays \n",
    "* Inspect the contents to see if the split performed as expected\n",
    "\n",
    "> Note: When randomly splitting data, it's always recommended to set a seed in order to ensure reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "# Set a seed\n",
    "np.random.seed(42)\n",
    "# Perform an 80/20 split\n",
    "# Make array of indices\n",
    "all_idx = np.arange(data.shape[0])\n",
    "# Randomly choose 80% subset of indices without replacement for training\n",
    "training_idx = np.random.choice(all_idx, size=round(546*.8), replace=False)\n",
    "# Choose remaining 20% of indices for testing\n",
    "test_idx = all_idx[~np.isin(all_idx, training_idx)]\n",
    "# Subset data \n",
    "training, test = data[training_idx,:], data[test_idx,:]\n",
    "\n",
    "# Check the shape of datasets\n",
    "print ('Raw data Shape: ', data.shape)\n",
    "print ('Train/Test Split:', training.shape, test.shape)\n",
    "\n",
    "# Create x and y for test and training sets\n",
    "x_train = training[:,:-1]\n",
    "y_train = training [:,-1]\n",
    "\n",
    "x_test = test[:,:-1]\n",
    "y_test = test[:,-1]\n",
    "\n",
    "# Check the shape of datasets\n",
    "print ('x_train, y_train, x_test, y_test:', x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# Split results\n",
    "# Raw data Shape:  (546, 13)\n",
    "# Train/Test Split: (437, 13) (109, 13)\n",
    "# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the `beta` \n",
    "\n",
    "With $X$ and $y$ in place, you can now compute your beta values with $x_\\text{train}$ and $y_\\text{train}$ as:\n",
    "#### $\\beta = (x_\\text{train}^T. x_\\text{train})^{-1} . x_\\text{train}^T . y_\\text{train}$\n",
    "\n",
    "* Using NumPy operations (transpose, inverse) that we saw earlier, compute the above equation in steps \n",
    "* Print your beta values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "# Calculate Xt.X and Xt.y for beta = (XT . X)-1 . XT . y - as seen in previous lessons\n",
    "Xt = np.transpose(x_train)\n",
    "XtX = np.dot(Xt,x_train)\n",
    "Xty = np.dot(Xt,y_train)\n",
    "\n",
    "# Calculate inverse of Xt.X\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "\n",
    "# Take the dot product of XtX_inv with Xty to compute beta\n",
    "beta = XtX_inv.dot(Xty)\n",
    "\n",
    "# Print the values of computed beta\n",
    "print(beta)\n",
    "\n",
    "\n",
    "\n",
    "# Beta values\n",
    "# Due to random split, your answers may vary \n",
    "# [-5.46637290e+03  3.62457767e+00  2.75100964e+03  1.47223649e+04\n",
    "#   5.97774591e+03  5.71916945e+03  5.73109882e+03  3.83586258e+03\n",
    "#   8.12674607e+03  1.33296437e+04  3.74995169e+03  1.01514699e+04]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Make predictions\n",
    "Great, you now have a set of coefficients that describe the linear mappings between $X$ and $y$. You can now use the calculated beta values with the test datasets that we left out to calculate $y$ predictions. Next, use all features in turn and multiply it with this beta. The result will give a prediction for each row which you can append to a new array of predictions.\n",
    "\n",
    "$\\hat{y} = x\\beta = \\beta_0 + \\beta_1 x_1 +  \\beta_2 x_2 + \\ldots + \\beta_m x_m $ \n",
    "\n",
    "* Create a new empty list (`y_pred`) for saving predictions\n",
    "* For each row of `x_test`, take the dot product of the row with beta to calculate the prediction for that row\n",
    "* Append the predictions to `y_pred`\n",
    "* Print the new set of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "# Calculate and print predictions for each row of X_test\n",
    "y_pred = []\n",
    "for row in x_test:\n",
    "    pred = row.dot(beta)\n",
    "    y_pred.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate model \n",
    "\n",
    "### Visualize actual vs. predicted values\n",
    "This is exciting, now your model can use the beta value to predict the price of houses given the input features. Let's plot these predictions against the actual values in `y_test` to see how much our model deviates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted and actual values as line plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(y_pred, linestyle='-', marker='o', label='predictions')\n",
    "plt.plot(y_test, linestyle='-', marker='o', label='actual values')\n",
    "plt.title('Actual vs. predicted values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look so bad, does it? Your model, although isn't perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could be a number of reasons for this. Let's try to dig a bit deeper to check model's predictive abilities by comparing these prediction with actual values of `y_test` individually. That will help you calculate the RMSE value (root mean squared error) for your model. \n",
    "\n",
    "### Root Mean Squared Error\n",
    "Here is the formula for RMSE:  \n",
    "\n",
    "$$ \\large RMSE = \\sqrt{\\sum^N_{i=1}\\dfrac{ (\\text{Predicted}_i-\\text{Actual}_i)^2}{N}}$$\n",
    "\n",
    "* Initialize an empty array `err`\n",
    "* For each row in `y_test` and `y_pred`, take the squared difference and append error for each row in the `err` array\n",
    "* Calculate $RMSE$ from `err` using the formula shown above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "err = []\n",
    "for pred,actual in zip(y_pred,y_test):\n",
    "    sq_err = (pred - actual) ** 2\n",
    "    err.append(sq_err)\n",
    "mean_sq_err = np.array(err).mean()\n",
    "root_mean_sq_err = np.sqrt(mean_sq_err)\n",
    "root_mean_sq_err\n",
    "\n",
    "# Due to random split, your answers may vary \n",
    "# RMSE = 14868.172645765708"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized root mean squared error\n",
    "The above error is clearly in terms of the dependent variable, i.e., the final house price. You can also use a normalized mean squared error in case of multiple regression which can be calculated from RMSE using following the formula:\n",
    "\n",
    "$$ \\large NRMSE = \\dfrac{RMSE}{max_i y_i - min_i y_i} $$\n",
    "\n",
    "* Calculate normalized RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NRMSE\n",
    "root_mean_sq_err/(y_train.max() - y_train.min())\n",
    "\n",
    "# Due to random split, your answers may vary \n",
    "# 0.09011013724706489"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is. A complete multiple regression analysis using nothing but NumPy. Having good programming skills in NumPy allows you to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques you saw here, you can easily build a whole neural network from scratch. \n",
    "\n",
    "## Level up (Optional)\n",
    "\n",
    "* Calculate the R-squared and adjusted R-squared for the above model \n",
    "* Plot the residuals (similar to `statsmodels`) and comment on the variance and heteroscedasticity\n",
    "* Run the experiment in `statsmodels` and compare the performance of both approaches in terms of computational cost\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you built a predictive model for predicting house prices. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. There are a number of shortcomings in this modeling approach and you can further apply a number of data modeling techniques to improve this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Complexity: From OLS to Gradient Descent\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson, you’ll be introduced to computational complexity. You'll learn about this idea in relationship with OLS regression and see how this may not be the most efficient algorithm to calculate the regression parameters when performing regression with large datasets. You'll set the stage for an optimization algorithm called \"Gradient Descent\" which will be covered in detail later. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Describe computational complexity and how it is related to Big O notation \n",
    "- Describe why OLS with matrix algebra would become problematic for large/complex data \n",
    "- Explain how optimizing techniques such as gradient descent can solve complexity issues\n",
    "\n",
    "## Complexities in OLS\n",
    "\n",
    "Recall the OLS formula for calculating the beta vector:\n",
    "\n",
    "$ \\beta =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y$\n",
    "\n",
    "\n",
    "This formula looks very simple, elegant, and intuitive. It works perfectly fine for the case of simple linear regression due to a limited number of computed dimensions, but with datasets that are very large or **big data** sets, it becomes computationally very expensive as it can potentially involve a huge number of complex mathematical operations.\n",
    "\n",
    "For this formula, we need to find $(\\boldsymbol{X}^T\\boldsymbol{X})$, and invert it as well, which makes it very expensive. Imagine the matrix $X_{(N \\times M+1)}$ has $(M+1)$ columns where $M$ is the number of predictors and $N$ is the number of rows of observations. In machine learning, you will often find datasets with $M >1000$ and $N > 1,000,000$. The $(\\boldsymbol{X}^T\\boldsymbol{X})$ matrix itself takes a while to calculate, then you have to invert an $M×M$ matrix which adds more to the complexity - making it very expensive. You'll also come across situations where the input matrix grows so large that it cannot fit into your computer's memory. \n",
    "\n",
    "## The Big O notation\n",
    "\n",
    "In computer science, Big O notation is used to describe how \"fast\" an algorithm grows, by comparing the number of operations within the algorithm. Big O notation helps you see the worst-case scenario for an algorithm. Typically, we are most concerned with the Big O time because we are interested in how slowly a given algorithm will possibly run at worst.\n",
    "\n",
    "#### Example\n",
    "Imagine you need to find a person you only know the name of. What's the most straightforward way of finding this person? Well, you could go through every single name in the phone book until you find your target. This is known as a simple search. If the phone book is not very long, with say, only 10 names, this is a fairly fast process. But what if there are 10,000 names in the phone book?\n",
    "\n",
    "At best, your target's name is at the front of the list and you only need to need to check the first item. At worst, your target's name is at the very end of the phone book and you will need to have searched all 10,000 names. As the \"dataset\" (or the phone book) increases in size, the maximum time it takes to run a simple search also linearly increases.\n",
    "\n",
    "Big O notation allows you to describe what the worst case is. The worst case is that you will have to search through all elements ($n$) in the phone book. You can describe the run-time as:\n",
    "\n",
    "**$O(n)$ where $n$ is the number of operations**\n",
    "\n",
    "Because the maximum number of operations is equal to the maximum number of elements in our phone book, we say the Big $O$ of a simple search is $O(n)$. **A simple search will never be slower than $O(n)$ time.**\n",
    "\n",
    "Different algorithms have different run-times. That is, algorithms grow at different rates. The most common Big O run-times, from fastest to slowest, are:\n",
    "\n",
    "* $O(\\log n)$: aka $\\log$ time\n",
    "* $O(n)$: aka linear time\n",
    "* $O(n^2)$\n",
    "* $O(n^3)$\n",
    "\n",
    "These rates, as well as some other rates, can be visualized in the following figure:\n",
    "\n",
    "<img src=\"images/big_o.png\" width =\"500\">\n",
    "\n",
    "### OLS and Big O notation\n",
    "\n",
    "Inverting a matrix costs $O(n^3)$ for computation where n is the number of rows in $X$ matrix, i.e., the observations. Here is an explanation of how to calculate Big O for OLS.\n",
    "\n",
    "OLS linear regression is computed as $(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y$.\n",
    "\n",
    "\n",
    "If $\\boldsymbol{X}$ is an $(n \\times k)$ matrix:\n",
    "\n",
    "- $(\\boldsymbol{X}^T\\boldsymbol{X})$ takes $O(n*k^2)$ time and produces a $(k \\times k)$ matrix\n",
    "- The matrix inversion of a (k x k) matrix takes $O(k^3)$ time\n",
    "- $(\\boldsymbol{X}^T\\boldsymbol{Y})$ takes $O(n*k^2)$ time and produces a $(k \\times k)$ matrix\n",
    "- The final matrix multiplication of two $(k \\times k)$ matrices takes $O(k^3)$ time\n",
    "\n",
    "\n",
    "** So the Big O running time for OLS is $O(k^{2*(n + k)})$ - which is pretty expensive **\n",
    "\n",
    "Moreover, if  $X$ is ill-conditioned  (i.e. it isn't a square matrix), there will be computational errors in the estimation. \n",
    "Another common problem is overfitting and underfitting in estimation of regression coefficients.\n",
    "\n",
    "So, this leads us to the gradient descent kind of optimization algorithm which can save us from this type of problem. The main reason why gradient descent is used for linear regression is the computational complexity: it's computationally cheaper (faster) to find the solution using the gradient descent in most cases. \n",
    "\n",
    "## Gradient Descent \n",
    "\n",
    "<img src=\"images/gradient_descent.png\" width =\"850\">\n",
    "\n",
    "\n",
    "> Gradient Descent is an iterative approach to minimize the model loss (error), used while training a machine learning model like linear regression. It is an optimization algorithm based on a convex function as shown in the figure above, that tweaks its parameters iteratively to minimize a given function to its local minimum.\n",
    "\n",
    "In regression, it is used to find the values of model parameters (coefficients, or the $\\beta$ matrix) that minimize a cost function (like RMSE) as far as possible.\n",
    "\n",
    "In order to fully understand how this works, you need to know what a gradient is and how is it calculated. And for this, you would need some Calculus. It may sound a bit intimidating at this stage, but don't worry. The next few sections will introduce you to the basics of calculus with gradients and derivatives. \n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Wiki: Computational complexity of mathematical operations](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations)\n",
    "\n",
    "- [Simplified Big O notation](https://medium.com/karuna-sehgal/a-simplified-explanation-of-the-big-o-notation-82523585e835)\n",
    "\n",
    "- [Gradient descent in a nutshell](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0)\n",
    "\n",
    "\n",
    "## Summary \n",
    "\n",
    "In this lesson, you learned about the shortcomings and limitations of OLS and matrix inverses. You looked at the Big O notation to explain how calculating inverses and transposes for large matrix might make our analysis unstable and computationally very expensive. This lesson sets a stage for your next section on calculus and gradient descent. You will have a much better understanding of the gradient descent diagram shown above and how it all works by the end of next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra - Recap\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this section, you learned the fundamentals of linear algebra. An understanding of linear algebra will help you better understand the underlying mathematics behind some machine learning algorithms.\n",
    "\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "The goal of this section was to provide both a conceptual and computational introduction to linear algebra - one of the foundational concepts underlying most machine learning models. Some of the key takeaways include: \n",
    "\n",
    "* One use case for vectors and matrices is for representing and solving systems of linear equations\n",
    "* A scalar is a single, real number. A vector is a one-dimensional array of numbers. A matrix is a 2-dimensional array of numbers \n",
    "* A tensor is a generalized term for an n-dimensional rectangular grid of numbers. A vector is a one-dimensional (first-order tensor), a matrix is a two-dimensional (second-order tensor), etc.\n",
    "* Two matrices can be added together if they have the same shape\n",
    "* Scalars can be added to matrices by adding the scalar (number) to each element\n",
    "* To calculate the dot product for matrix multiplication, the first matrix must have the same number of columns as the number of rows in the second matrix \n",
    "* Operating on NumPy data types is substantially more computationally efficient than performing the same operations on native Python data types\n",
    "* It is possible to use linear algebra in NumPy to solve for a linear regression using the OLS method\n",
    "* OLS is not computationally efficient, so in practice, we usually perform a gradient descent instead to solve a linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Addition and Broadcasting in Numpy - Code Along\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This lesson is a supplement to the previous lesson where you learned how to create Numpy arrays as vectors and matrices. In this lesson, you'll look at matrix addition and broadcasting features offered by Numpy.\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "- Implement vector addition in Numpy \n",
    "- Describe how broadcasting differs from addition if there are mismatched dimensions \n",
    "\n",
    "\n",
    "## Vector addition\n",
    "\n",
    "Let's look at simple vector addition, where all operations are performed element-wise between two vectors/matrices of equal size to result in a new vector/matrix with the same size.\n",
    "\n",
    "Imagine two arrays A and B with the same dimensions. They can be added together if: \n",
    "\n",
    "* they have the same shape\n",
    "* each cell of A is added to the corresponding cell of B\n",
    "\n",
    "$A_{i,j} +B_{i,j} = C_{i,j}$\n",
    "\n",
    "\n",
    "$$ C=\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1} & A_{1,2} \\\\\n",
    "   A_{2,1} & A_{2,2} \\\\\n",
    "   A_{3,1} & A_{3,2} \\\\\n",
    "  \\end{array} } \\right] +\n",
    "    \\left[ {\\begin{array}{cc}\n",
    "   B_{1,1} & B_{1,2} \\\\\n",
    "   B_{2,1} & B_{2,2} \\\\\n",
    "   B_{3,1} & B_{3,2} \\\\\n",
    "  \\end{array} } \\right] =\n",
    "   \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1} + B_{1,1} & A_{1,2} + B_{1,2}\\\\\n",
    "   A_{2,1} + B_{2,1}& A_{2,2} + B_{2,2} \\\\\n",
    "   A_{3,1} + B_{3,1} & A_{3,2} + B_{3,2} \\\\\n",
    "  \\end{array} } \\right] \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "here $A_(i, j)$ and $B_(i, j)$ represent row and column locations. This is a more standard notation that you will find in most literature. Another visual representation is shown below:\n",
    "<img src=\"images/new_addition.png\" width=\"250\">\n",
    "\n",
    "\n",
    "1D-arrays can be added together in exactly the same way using similar assumptions. The addition of two vectors $x$ and $y$ may be represented graphically by placing the start of the arrow y at the tip of the arrow x, and then drawing an arrow from the start (tail) of $x$ to the tip (head) of $y$. The new arrow represents the vector $x + y$.\n",
    "<img src=\"images/new_vector_addition.png\" width=\"350\">\n",
    "\n",
    "You can perform addition operations in Numpy in the following way:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Adding 1D arrays\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6]) \n",
    "c = a + b\n",
    "c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting a vector is the same as adding its negative. So, the difference of the vectors x and y is equal to the sum of x and -y: \n",
    "> $x - y = x + (-y)$\n",
    "\n",
    "Geometrically, when we subtract y from x, we place the end points of x and y at the same point, and then draw an arrow from the tip of y to the tip of x. That arrow represents the vector x - y.\n",
    "<img src=\"images/new_vector_subtraction.png\" width=\"400\">\n",
    "\n",
    "Mathematically, we subtract the corresponding components of vector y from the vector x: \n",
    "\n",
    "```python\n",
    "# Subtracting 1D arrays\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6]) \n",
    "c = b - a\n",
    "c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Adding 2D matrices\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "B = np.array([[1, 4], [2, 5], [2, 3]])\n",
    "# Add matrices A and B\n",
    "C = A + B\n",
    "C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Add matrices with mismatched dimensions\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "B = np.array([[1, 4], [2, 5]])\n",
    "# Add matrices A and B\n",
    "C = A + B\n",
    "C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You received an error, as expected, because there is a dimension mismatch. Here, it seems intuitive to know why this happened, but when working with large matrices and tensors, shape mismatch could become a real problem and as data scientists, you need to make sure to be aware about the dimensions of your datasets.\n",
    "\n",
    "## Vector scalar addition\n",
    "\n",
    "Scalar values can be added to matrices and vectors. In this case, the scalar value is added to each element of array as shown below:\n",
    "```python\n",
    "# Add scalars to arrays\n",
    "# Add a scalar to a 1D vector\n",
    "print(a + 4)\n",
    "# Add a scalar to a 2D matrix\n",
    "print(A + 4)\n",
    "```\n",
    "\n",
    "## Broadcasting\n",
    "\n",
    "Numpy can also handle operations on arrays of different shapes as some machine learning algorithms need that. The smaller array gets **extended** to match the shape of the larger array. In the scalar-vector addition, we used broadcasting so the scalar was converted in an array of same shape as $A$.\n",
    "\n",
    "\n",
    "$$ \n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1} & A_{1,2} \\\\\n",
    "   A_{2,1} & A_{2,2} \\\\\n",
    "   A_{3,1} & A_{3,2} \\\\\n",
    "  \\end{array} } \\right] +\n",
    "    \\left[ {\\begin{array}{c}\n",
    "   B_{1,1}\\\\\n",
    "   B_{2,1}\\\\\n",
    "   B_{3,1}\\\\\n",
    "   \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1} & A_{1,2} \\\\\n",
    "   A_{2,1} & A_{2,2} \\\\\n",
    "   A_{3,1} & A_{3,2} \\\\\n",
    "  \\end{array} } \\right] +\n",
    "    \\left[ {\\begin{array}{cc}\n",
    "   B_{1,1} & B_{1,1} \\\\\n",
    "   B_{2,1} & B_{2,1} \\\\\n",
    "   B_{3,1} & B_{3,1} \\\\\n",
    "  \\end{array} } \\right] =\n",
    "   \\left[ {\\begin{array}{cc}\n",
    "   A_{1,1} + B_{1,1} & A_{1,2} + B_{1,1}\\\\\n",
    "   A_{2,1} + B_{2,1}& A_{2,2} + B_{2,1} \\\\\n",
    "   A_{3,1} + B_{3,1} & A_{3,2} + B_{3,1} \\\\\n",
    "  \\end{array} } \\right] \n",
    "$$\n",
    "\n",
    "Let's see this in action while trying to add arrays with different shapes\n",
    "\n",
    "```python\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "B = np.array([[2], [4], [6]])\n",
    "print(B)\n",
    "A + B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson, you learned how to add vectors and matrices and looked at the dimension match assumption necessary for this addition. You also looked at how Numpy allows you to use broadcasting to add scalars and vector/matrices to other objects with different dimensions. In the following lessons, you'll learn about more complicated mathematical operations and their use in real life data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
